{"cells":[{"cell_type":"markdown","metadata":{"id":"r6vgge28bsTY"},"source":["# Connect colab to Google Drive"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"lf6cGx5PRHCQ","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f6ba103d-bb93-4fd9-f41f-7cfe8d68a9f4","executionInfo":{"status":"ok","timestamp":1643477246354,"user_tz":300,"elapsed":21005,"user":{"displayName":"Wolfgang Gruen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01742609913198784862"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-01-29 17:27:05--  https://raw.githubusercontent.com/wgruen/lizard/master/mount_gdrive.ipynb\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 10889 (11K) [text/plain]\n","Saving to: ‘mount_gdrive.ipynb’\n","\n","\rmount_gdrive.ipynb    0%[                    ]       0  --.-KB/s               \rmount_gdrive.ipynb  100%[===================>]  10.63K  --.-KB/s    in 0s      \n","\n","2022-01-29 17:27:05 (80.4 MB/s) - ‘mount_gdrive.ipynb’ saved [10889/10889]\n","\n","Mounted at /content/drive\n","drive  mount_gdrive.ipynb  sample_data\n","/content\n","physical devises\n","[]\n","tensorflow version\n","2.7.0\n","keras version\n","2.7.0\n","Not connected to a GPU\n","Your runtime has 13.6 gigabytes of available RAM\n","\n","Not using a high-RAM runtime\n"]}],"source":["!wget -O mount_gdrive.ipynb https://raw.githubusercontent.com/wgruen/lizard/master/mount_gdrive.ipynb\n","\n","#!ls\n","%run mount_gdrive.ipynb"]},{"cell_type":"markdown","source":["# Run the machine"],"metadata":{"id":"Nwhz-LWcOW1R"}},{"cell_type":"markdown","source":["This code has not been started yet."],"metadata":{"id":"65zqbtNe6t8F"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"PmnklYq65Cx6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643477288483,"user_tz":300,"elapsed":32306,"user":{"displayName":"Wolfgang Gruen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01742609913198784862"}},"outputId":"10fa1e7e-be79-4c81-8432-f9572c4cd13e"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard\n","diff.log\t   get_tpu.ipynb       picture_problem\n","first_neural_nets  mount_gdrive.ipynb  README.md\n","get_gpu.ipynb\t   nmf\t\t       statistical_analysis\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression\n","configuration.yaml\t\t    linear_regression_basic_example.py\toutput\n","linear_regression_analyze_tp.ipynb  linear_regression_example.ipynb\n","linear_regression_analyze_tp.py     longley_example.py\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression\n","Collecting reportlab\n","  Downloading reportlab-3.6.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n","\u001b[K     |████████████████████████████████| 2.8 MB 12.0 MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from reportlab) (7.1.2)\n","Installing collected packages: reportlab\n","Successfully installed reportlab-3.6.6\n","args:  Namespace(input_config_file='configuration.yaml')\n","Description: 'The test is about analysing a file or files in a direcotry, then creating\n","  pdf files read_file_or_dir \"../data/D1-150.csv\" read_file_or_dir \"../data/\"\n","\n","  The percentage offset for interquartile should be 25, since it is supposed to be\n","  25 percent and 75 percent'\n","cutoff_offset_percent: 25\n","read_file_or_dir: ../data/\n","\n","D1-150.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D1-150.csv\n","data \n"," [[  1. 123. 124. ...  92. 113.  96.]\n"," [  2.  90.  89. ...  98.  71.   0.]\n"," [  3.  89.  87. ...  77.  68. 108.]\n"," ...\n"," [148.  76.  89. ... 182.  81. 124.]\n"," [149.  94. 105. ...  61.  13. 242.]\n"," [150.  95. 100. ...  57.  70. 221.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n","Y \n"," [[123. 124. 102. ...  92. 113.  96.]\n"," [ 90.  89. 103. ...  98.  71.   0.]\n"," [ 89.  87.  80. ...  77.  68. 108.]\n"," ...\n"," [ 76.  89.  65. ... 182.  81. 124.]\n"," [ 94. 105. 101. ...  61.  13. 242.]\n"," [ 95. 100. 101. ...  57.  70. 221.]]\n","array([[123., 124., 102., ...,  92., 113.,  96.],\n","       [ 90.,  89., 103., ...,  98.,  71.,   0.],\n","       [ 89.,  87.,  80., ...,  77.,  68., 108.],\n","       ...,\n","       [ 76.,  89.,  65., ..., 182.,  81., 124.],\n","       [ 94., 105., 101., ...,  61.,  13., 242.],\n","       [ 95., 100., 101., ...,  57.,  70., 221.]])\n","len           X / Y:  150 150\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","len flattened X / Y:  1500 1500\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","Y flattened \n"," [123. 124. 102. ...  57.  70. 221.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  69.0\n","quartile_high:  125.0\n","interquartile_range:  56.0\n","Percentiles: \n","25th=69.0\n","75th=125.0\n","interquartile range: 56.0\n","Identified outliers: 39\n","Non-outlier observations: 1461\n","Identified outliers: 39\n","Outliers: \n"," [341.0, 236.0, 324.0, 214.0, 249.0, 213.0, 212.0, 216.0, 283.0, 289.0, 327.0, 387.0, 231.0, 338.0, 605.0, 340.0, 588.0, 309.0, 242.0, 227.0, 210.0, 238.0, 214.0, 236.0, 234.0, 284.0, 291.0, 343.0, 508.0, 251.0, 237.0, 928.0, 472.0, 308.0, 542.0, 288.0, 424.0, 242.0, 221.0]\n","Y_outliers: \n"," [ nan  nan  nan ...  nan  nan 221.]\n","len pos_outliers: \n"," 40\n","len pos_zeros: \n"," 73\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...        -15.0         209.0\n","\n","[1 rows x 8 columns]\n","D1-250.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D1-250.csv\n","data \n"," [[  1. 201. 187. ...   0.   0.   0.]\n"," [  2.  75.  59. ...  95. 165. 155.]\n"," [  3. 158. 167. ... 141.   0.   0.]\n"," ...\n"," [248.  51.  60. ... 109. 124. 213.]\n"," [249.  52.  60. ...  69.   0. 424.]\n"," [250. 109. 100. ... 115.   0. 221.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n","Y \n"," [[201. 187. 198. ...   0.   0.   0.]\n"," [ 75.  59.  64. ...  95. 165. 155.]\n"," [158. 167. 101. ... 141.   0.   0.]\n"," ...\n"," [ 51.  60.  69. ... 109. 124. 213.]\n"," [ 52.  60.  64. ...  69.   0. 424.]\n"," [109. 100. 106. ... 115.   0. 221.]]\n","array([[201., 187., 198., ...,   0.,   0.,   0.],\n","       [ 75.,  59.,  64., ...,  95., 165., 155.],\n","       [158., 167., 101., ..., 141.,   0.,   0.],\n","       ...,\n","       [ 51.,  60.,  69., ..., 109., 124., 213.],\n","       [ 52.,  60.,  64., ...,  69.,   0., 424.],\n","       [109., 100., 106., ..., 115.,   0., 221.]])\n","len           X / Y:  250 250\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","len flattened X / Y:  2500 2500\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","Y flattened \n"," [201. 187. 198. ... 115.   0. 221.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  59.0\n","quartile_high:  133.0\n","interquartile_range:  74.0\n","Percentiles: \n","25th=59.0\n","75th=133.0\n","interquartile range: 74.0\n","Identified outliers: 59\n","Non-outlier observations: 2441\n","Identified outliers: 59\n","Outliers: \n"," [297.0, 352.0, 487.0, 269.0, 327.0, 551.0, 561.0, 245.0, 430.0, 355.0, 249.0, 262.0, 273.0, 246.0, 245.0, 630.0, 253.0, 260.0, 330.0, 267.0, 266.0, 707.0, 341.0, 254.0, 413.0, 304.0, 292.0, 273.0, 260.0, 293.0, 824.0, 632.0, 336.0, 346.0, 266.0, 325.0, 275.0, 344.0, 845.0, 293.0, 765.0, 308.0, 272.0, 334.0, 286.0, 323.0, 318.0, 370.0, 449.0, 251.0, 658.0, 326.0, 438.0, 289.0, 409.0, 256.0, 303.0, 405.0, 424.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 60\n","len pos_zeros: \n"," 269\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...        -15.0         209.0\n","1  ../data/D1-250.csv         60  ...        -52.0         244.0\n","\n","[2 rows x 8 columns]\n","D1-400.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D1-400.csv\n","data \n"," [[  1. 251. 282. ...   0.   0.   0.]\n"," [  2. 123. 135. ... 148.   0.   0.]\n"," [  3. 220. 227. ...   0.   0.   0.]\n"," ...\n"," [398. 127. 146. ...   0.   0. 286.]\n"," [399.  84.  89. ... 160. 179.  46.]\n"," [400. 168. 184. ...   0.   0.  89.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n"," 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266.\n"," 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280.\n"," 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294.\n"," 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308.\n"," 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322.\n"," 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336.\n"," 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350.\n"," 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363. 364.\n"," 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377. 378.\n"," 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391. 392.\n"," 393. 394. 395. 396. 397. 398. 399. 400.]\n","Y \n"," [[251. 282. 250. ...   0.   0.   0.]\n"," [123. 135. 127. ... 148.   0.   0.]\n"," [220. 227. 186. ...   0.   0.   0.]\n"," ...\n"," [127. 146. 165. ...   0.   0. 286.]\n"," [ 84.  89.  93. ... 160. 179.  46.]\n"," [168. 184. 196. ...   0.   0.  89.]]\n","array([[251., 282., 250., ...,   0.,   0.,   0.],\n","       [123., 135., 127., ..., 148.,   0.,   0.],\n","       [220., 227., 186., ...,   0.,   0.,   0.],\n","       ...,\n","       [127., 146., 165., ...,   0.,   0., 286.],\n","       [ 84.,  89.,  93., ..., 160., 179.,  46.],\n","       [168., 184., 196., ...,   0.,   0.,  89.]])\n","len           X / Y:  400 400\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","len flattened X / Y:  4000 4000\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","Y flattened \n"," [251. 282. 250. ...   0.   0.  89.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  40.0\n","quartile_high:  143.0\n","interquartile_range:  103.0\n","Percentiles: \n","25th=40.0\n","75th=143.0\n","interquartile range: 103.0\n","Identified outliers: 100\n","Non-outlier observations: 3900\n","Identified outliers: 100\n","Outliers: \n"," [305.0, 312.0, 314.0, 315.0, 308.0, 331.0, 340.0, 341.0, 341.0, 331.0, 328.0, 322.0, 335.0, 336.0, 327.0, 325.0, 312.0, 309.0, 308.0, 311.0, 310.0, 302.0, 303.0, 335.0, 392.0, 395.0, 425.0, 470.0, 330.0, 489.0, 492.0, 322.0, 383.0, 520.0, 430.0, 336.0, 302.0, 315.0, 305.0, 326.0, 316.0, 359.0, 324.0, 476.0, 324.0, 309.0, 328.0, 615.0, 328.0, 318.0, 331.0, 327.0, 627.0, 367.0, 377.0, 368.0, 346.0, 349.0, 338.0, 662.0, 319.0, 312.0, 308.0, 706.0, 321.0, 698.0, 652.0, 811.0, 398.0, 746.0, 744.0, 900.0, 647.0, 599.0, 308.0, 315.0, 909.0, 931.0, 477.0, 306.0, 396.0, 508.0, 437.0, 338.0, 493.0, 1000.0, 401.0, 319.0, 1000.0, 1000.0, 607.0, 452.0, 382.0, 1000.0, 370.0, 759.0, 435.0, 955.0, 962.0, 891.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 100\n","len pos_zeros: \n"," 826\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...        -15.0         209.0\n","1  ../data/D1-250.csv         60  ...        -52.0         244.0\n","2  ../data/D1-400.csv        100  ...       -114.5         297.5\n","\n","[3 rows x 8 columns]\n","D1-50.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D1-50.csv\n","data \n"," [[  1. 111. 110. 115.  97.  79.  95.  85. 111. 103.  94.]\n"," [  2.  84.  83.  79.  78.  69.  74.  96. 134. 175. 128.]\n"," [  3. 107. 119. 127. 126.  87.  93.  76.  99.  66. 100.]\n"," [  4. 110.  99. 110. 111. 109. 127. 134.  71. 113.  16.]\n"," [  5.  95. 111. 115. 105. 113. 108. 134. 100.  97.  22.]\n"," [  6.  72.  84.  94.  93.  92. 126. 118. 122. 119.  80.]\n"," [  7. 110. 116.  96. 107.  78.  88.  78.  93. 122. 112.]\n"," [  8. 105. 114. 121. 130. 146. 116. 131.  82.  55.   0.]\n"," [  9.  75.  76.  77.  67.  67.  60.  73.  97. 117. 291.]\n"," [ 10.  97. 100. 107. 103. 102. 117.  94. 103.  56. 121.]\n"," [ 11.  90.  91.  94. 103.  93.  78.  80.  81.  99. 191.]\n"," [ 12.  71.  66.  77.  71.  76.  87.  75. 111.  73. 293.]\n"," [ 13.  92.  94.  89.  92. 106.  86.  98. 114. 105. 124.]\n"," [ 14. 103. 107. 115. 119. 128. 104.  93. 111.  52.  68.]\n"," [ 15. 151. 147. 126. 115. 109.  92. 117.  88.  55.   0.]\n"," [ 16.  97.  98.  87.  91. 111. 124. 105. 127.  86.  74.]\n"," [ 17. 106. 103.  93. 128.  99. 103. 126. 129.  81.  32.]\n"," [ 18. 119. 106. 113.  99. 105. 105.  98.  73.  46. 136.]\n"," [ 19.  69.  70.  67.  72. 103. 119. 127.  86.  89. 198.]\n"," [ 20. 123. 119. 125. 143. 109. 123.  91.  64.  75.  28.]\n"," [ 21.  80.  79.  79.  92.  88. 100.  99. 116. 168.  99.]\n"," [ 22. 147. 133. 124. 127. 126. 110. 121.  86.  26.   0.]\n"," [ 23.  89.  99. 101.  94. 106. 109. 144.  99. 114.  45.]\n"," [ 24. 115. 107. 113.  98. 121. 105. 100.  99.  68.  74.]\n"," [ 25.  79.  86.  83.  95. 104. 127. 129. 139. 113.  45.]\n"," [ 26. 102. 102. 106. 122. 107.  93.  93. 125. 105.  45.]\n"," [ 27.  79.  86.  74.  73.  68.  54.  69. 139. 128. 230.]\n"," [ 28.  76.  83.  90.  93. 110.  98. 104.  84.  86. 176.]\n"," [ 29.  93.  86.  79.  85.  69.  99.  82. 147. 121. 139.]\n"," [ 30. 107. 105. 116. 100. 125. 109.  96. 105.  65.  72.]\n"," [ 31.  74.  82.  92.  88.  86.  83.  72. 110. 181. 132.]\n"," [ 32. 124. 102.  95.  84.  73.  73. 113.  63. 178.  95.]\n"," [ 33. 132. 129. 139. 122. 109. 109.  82.  70.  81.  27.]\n"," [ 34.  98.  96.  84. 100.  87. 116. 100. 117. 113.  89.]\n"," [ 35. 161. 145. 127. 106. 123.  93.  65.  74.  85.  21.]\n"," [ 36. 114. 114. 124. 102.  90.  80. 118. 105.  84.  69.]\n"," [ 37.  64.  65.  68.  70.  91. 103. 154. 126. 153. 106.]\n"," [ 38.  94.  92.  90.  91. 101.  95.  71.  80.  94. 192.]\n"," [ 39. 131. 137. 120. 119. 119. 103.  86.  92.  93.   0.]\n"," [ 40. 122. 126. 128. 125. 122.  74.  72.  75. 108.  48.]\n"," [ 41. 112. 104. 101.  99. 108. 106. 125. 106. 108.  31.]\n"," [ 42.  93. 110. 102. 109. 129. 111. 104.  95.  99.  48.]\n"," [ 43.  90. 110. 101. 117.  89. 123. 103.  92. 122.  53.]\n"," [ 44.  74.  72.  76.  71.  88. 107. 109. 125. 131. 147.]\n"," [ 45. 130. 120. 127. 146. 152. 122.  51.  27.  26.  99.]\n"," [ 46.  71.  80.  87.  81.  72.  70.  82.  84.  97. 276.]\n"," [ 47. 108. 100.  89.  87.  76.  71.  84.  91. 122. 172.]\n"," [ 48.  84.  82.  87.  87.  96. 104.  92.  96.  93. 179.]\n"," [ 49.  86.  85.  86. 101. 109. 118. 117. 110.  96.  92.]\n"," [ 50.  84.  70.  85.  66.  75. 110. 134. 127. 158.  91.]]\n","X \n"," [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n"," 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n"," 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n","Y \n"," [[111. 110. 115.  97.  79.  95.  85. 111. 103.  94.]\n"," [ 84.  83.  79.  78.  69.  74.  96. 134. 175. 128.]\n"," [107. 119. 127. 126.  87.  93.  76.  99.  66. 100.]\n"," [110.  99. 110. 111. 109. 127. 134.  71. 113.  16.]\n"," [ 95. 111. 115. 105. 113. 108. 134. 100.  97.  22.]\n"," [ 72.  84.  94.  93.  92. 126. 118. 122. 119.  80.]\n"," [110. 116.  96. 107.  78.  88.  78.  93. 122. 112.]\n"," [105. 114. 121. 130. 146. 116. 131.  82.  55.   0.]\n"," [ 75.  76.  77.  67.  67.  60.  73.  97. 117. 291.]\n"," [ 97. 100. 107. 103. 102. 117.  94. 103.  56. 121.]\n"," [ 90.  91.  94. 103.  93.  78.  80.  81.  99. 191.]\n"," [ 71.  66.  77.  71.  76.  87.  75. 111.  73. 293.]\n"," [ 92.  94.  89.  92. 106.  86.  98. 114. 105. 124.]\n"," [103. 107. 115. 119. 128. 104.  93. 111.  52.  68.]\n"," [151. 147. 126. 115. 109.  92. 117.  88.  55.   0.]\n"," [ 97.  98.  87.  91. 111. 124. 105. 127.  86.  74.]\n"," [106. 103.  93. 128.  99. 103. 126. 129.  81.  32.]\n"," [119. 106. 113.  99. 105. 105.  98.  73.  46. 136.]\n"," [ 69.  70.  67.  72. 103. 119. 127.  86.  89. 198.]\n"," [123. 119. 125. 143. 109. 123.  91.  64.  75.  28.]\n"," [ 80.  79.  79.  92.  88. 100.  99. 116. 168.  99.]\n"," [147. 133. 124. 127. 126. 110. 121.  86.  26.   0.]\n"," [ 89.  99. 101.  94. 106. 109. 144.  99. 114.  45.]\n"," [115. 107. 113.  98. 121. 105. 100.  99.  68.  74.]\n"," [ 79.  86.  83.  95. 104. 127. 129. 139. 113.  45.]\n"," [102. 102. 106. 122. 107.  93.  93. 125. 105.  45.]\n"," [ 79.  86.  74.  73.  68.  54.  69. 139. 128. 230.]\n"," [ 76.  83.  90.  93. 110.  98. 104.  84.  86. 176.]\n"," [ 93.  86.  79.  85.  69.  99.  82. 147. 121. 139.]\n"," [107. 105. 116. 100. 125. 109.  96. 105.  65.  72.]\n"," [ 74.  82.  92.  88.  86.  83.  72. 110. 181. 132.]\n"," [124. 102.  95.  84.  73.  73. 113.  63. 178.  95.]\n"," [132. 129. 139. 122. 109. 109.  82.  70.  81.  27.]\n"," [ 98.  96.  84. 100.  87. 116. 100. 117. 113.  89.]\n"," [161. 145. 127. 106. 123.  93.  65.  74.  85.  21.]\n"," [114. 114. 124. 102.  90.  80. 118. 105.  84.  69.]\n"," [ 64.  65.  68.  70.  91. 103. 154. 126. 153. 106.]\n"," [ 94.  92.  90.  91. 101.  95.  71.  80.  94. 192.]\n"," [131. 137. 120. 119. 119. 103.  86.  92.  93.   0.]\n"," [122. 126. 128. 125. 122.  74.  72.  75. 108.  48.]\n"," [112. 104. 101.  99. 108. 106. 125. 106. 108.  31.]\n"," [ 93. 110. 102. 109. 129. 111. 104.  95.  99.  48.]\n"," [ 90. 110. 101. 117.  89. 123. 103.  92. 122.  53.]\n"," [ 74.  72.  76.  71.  88. 107. 109. 125. 131. 147.]\n"," [130. 120. 127. 146. 152. 122.  51.  27.  26.  99.]\n"," [ 71.  80.  87.  81.  72.  70.  82.  84.  97. 276.]\n"," [108. 100.  89.  87.  76.  71.  84.  91. 122. 172.]\n"," [ 84.  82.  87.  87.  96. 104.  92.  96.  93. 179.]\n"," [ 86.  85.  86. 101. 109. 118. 117. 110.  96.  92.]\n"," [ 84.  70.  85.  66.  75. 110. 134. 127. 158.  91.]]\n","array([[111., 110., 115.,  97.,  79.,  95.,  85., 111., 103.,  94.],\n","       [ 84.,  83.,  79.,  78.,  69.,  74.,  96., 134., 175., 128.],\n","       [107., 119., 127., 126.,  87.,  93.,  76.,  99.,  66., 100.],\n","       [110.,  99., 110., 111., 109., 127., 134.,  71., 113.,  16.],\n","       [ 95., 111., 115., 105., 113., 108., 134., 100.,  97.,  22.],\n","       [ 72.,  84.,  94.,  93.,  92., 126., 118., 122., 119.,  80.],\n","       [110., 116.,  96., 107.,  78.,  88.,  78.,  93., 122., 112.],\n","       [105., 114., 121., 130., 146., 116., 131.,  82.,  55.,   0.],\n","       [ 75.,  76.,  77.,  67.,  67.,  60.,  73.,  97., 117., 291.],\n","       [ 97., 100., 107., 103., 102., 117.,  94., 103.,  56., 121.],\n","       [ 90.,  91.,  94., 103.,  93.,  78.,  80.,  81.,  99., 191.],\n","       [ 71.,  66.,  77.,  71.,  76.,  87.,  75., 111.,  73., 293.],\n","       [ 92.,  94.,  89.,  92., 106.,  86.,  98., 114., 105., 124.],\n","       [103., 107., 115., 119., 128., 104.,  93., 111.,  52.,  68.],\n","       [151., 147., 126., 115., 109.,  92., 117.,  88.,  55.,   0.],\n","       [ 97.,  98.,  87.,  91., 111., 124., 105., 127.,  86.,  74.],\n","       [106., 103.,  93., 128.,  99., 103., 126., 129.,  81.,  32.],\n","       [119., 106., 113.,  99., 105., 105.,  98.,  73.,  46., 136.],\n","       [ 69.,  70.,  67.,  72., 103., 119., 127.,  86.,  89., 198.],\n","       [123., 119., 125., 143., 109., 123.,  91.,  64.,  75.,  28.],\n","       [ 80.,  79.,  79.,  92.,  88., 100.,  99., 116., 168.,  99.],\n","       [147., 133., 124., 127., 126., 110., 121.,  86.,  26.,   0.],\n","       [ 89.,  99., 101.,  94., 106., 109., 144.,  99., 114.,  45.],\n","       [115., 107., 113.,  98., 121., 105., 100.,  99.,  68.,  74.],\n","       [ 79.,  86.,  83.,  95., 104., 127., 129., 139., 113.,  45.],\n","       [102., 102., 106., 122., 107.,  93.,  93., 125., 105.,  45.],\n","       [ 79.,  86.,  74.,  73.,  68.,  54.,  69., 139., 128., 230.],\n","       [ 76.,  83.,  90.,  93., 110.,  98., 104.,  84.,  86., 176.],\n","       [ 93.,  86.,  79.,  85.,  69.,  99.,  82., 147., 121., 139.],\n","       [107., 105., 116., 100., 125., 109.,  96., 105.,  65.,  72.],\n","       [ 74.,  82.,  92.,  88.,  86.,  83.,  72., 110., 181., 132.],\n","       [124., 102.,  95.,  84.,  73.,  73., 113.,  63., 178.,  95.],\n","       [132., 129., 139., 122., 109., 109.,  82.,  70.,  81.,  27.],\n","       [ 98.,  96.,  84., 100.,  87., 116., 100., 117., 113.,  89.],\n","       [161., 145., 127., 106., 123.,  93.,  65.,  74.,  85.,  21.],\n","       [114., 114., 124., 102.,  90.,  80., 118., 105.,  84.,  69.],\n","       [ 64.,  65.,  68.,  70.,  91., 103., 154., 126., 153., 106.],\n","       [ 94.,  92.,  90.,  91., 101.,  95.,  71.,  80.,  94., 192.],\n","       [131., 137., 120., 119., 119., 103.,  86.,  92.,  93.,   0.],\n","       [122., 126., 128., 125., 122.,  74.,  72.,  75., 108.,  48.],\n","       [112., 104., 101.,  99., 108., 106., 125., 106., 108.,  31.],\n","       [ 93., 110., 102., 109., 129., 111., 104.,  95.,  99.,  48.],\n","       [ 90., 110., 101., 117.,  89., 123., 103.,  92., 122.,  53.],\n","       [ 74.,  72.,  76.,  71.,  88., 107., 109., 125., 131., 147.],\n","       [130., 120., 127., 146., 152., 122.,  51.,  27.,  26.,  99.],\n","       [ 71.,  80.,  87.,  81.,  72.,  70.,  82.,  84.,  97., 276.],\n","       [108., 100.,  89.,  87.,  76.,  71.,  84.,  91., 122., 172.],\n","       [ 84.,  82.,  87.,  87.,  96., 104.,  92.,  96.,  93., 179.],\n","       [ 86.,  85.,  86., 101., 109., 118., 117., 110.,  96.,  92.],\n","       [ 84.,  70.,  85.,  66.,  75., 110., 134., 127., 158.,  91.]])\n","len           X / Y:  50 50\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","len flattened X / Y:  500 500\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","Y flattened \n"," [111. 110. 115.  97.  79.  95.  85. 111. 103.  94.  84.  83.  79.  78.\n","  69.  74.  96. 134. 175. 128. 107. 119. 127. 126.  87.  93.  76.  99.\n","  66. 100. 110.  99. 110. 111. 109. 127. 134.  71. 113.  16.  95. 111.\n"," 115. 105. 113. 108. 134. 100.  97.  22.  72.  84.  94.  93.  92. 126.\n"," 118. 122. 119.  80. 110. 116.  96. 107.  78.  88.  78.  93. 122. 112.\n"," 105. 114. 121. 130. 146. 116. 131.  82.  55.   0.  75.  76.  77.  67.\n","  67.  60.  73.  97. 117. 291.  97. 100. 107. 103. 102. 117.  94. 103.\n","  56. 121.  90.  91.  94. 103.  93.  78.  80.  81.  99. 191.  71.  66.\n","  77.  71.  76.  87.  75. 111.  73. 293.  92.  94.  89.  92. 106.  86.\n","  98. 114. 105. 124. 103. 107. 115. 119. 128. 104.  93. 111.  52.  68.\n"," 151. 147. 126. 115. 109.  92. 117.  88.  55.   0.  97.  98.  87.  91.\n"," 111. 124. 105. 127.  86.  74. 106. 103.  93. 128.  99. 103. 126. 129.\n","  81.  32. 119. 106. 113.  99. 105. 105.  98.  73.  46. 136.  69.  70.\n","  67.  72. 103. 119. 127.  86.  89. 198. 123. 119. 125. 143. 109. 123.\n","  91.  64.  75.  28.  80.  79.  79.  92.  88. 100.  99. 116. 168.  99.\n"," 147. 133. 124. 127. 126. 110. 121.  86.  26.   0.  89.  99. 101.  94.\n"," 106. 109. 144.  99. 114.  45. 115. 107. 113.  98. 121. 105. 100.  99.\n","  68.  74.  79.  86.  83.  95. 104. 127. 129. 139. 113.  45. 102. 102.\n"," 106. 122. 107.  93.  93. 125. 105.  45.  79.  86.  74.  73.  68.  54.\n","  69. 139. 128. 230.  76.  83.  90.  93. 110.  98. 104.  84.  86. 176.\n","  93.  86.  79.  85.  69.  99.  82. 147. 121. 139. 107. 105. 116. 100.\n"," 125. 109.  96. 105.  65.  72.  74.  82.  92.  88.  86.  83.  72. 110.\n"," 181. 132. 124. 102.  95.  84.  73.  73. 113.  63. 178.  95. 132. 129.\n"," 139. 122. 109. 109.  82.  70.  81.  27.  98.  96.  84. 100.  87. 116.\n"," 100. 117. 113.  89. 161. 145. 127. 106. 123.  93.  65.  74.  85.  21.\n"," 114. 114. 124. 102.  90.  80. 118. 105.  84.  69.  64.  65.  68.  70.\n","  91. 103. 154. 126. 153. 106.  94.  92.  90.  91. 101.  95.  71.  80.\n","  94. 192. 131. 137. 120. 119. 119. 103.  86.  92.  93.   0. 122. 126.\n"," 128. 125. 122.  74.  72.  75. 108.  48. 112. 104. 101.  99. 108. 106.\n"," 125. 106. 108.  31.  93. 110. 102. 109. 129. 111. 104.  95.  99.  48.\n","  90. 110. 101. 117.  89. 123. 103.  92. 122.  53.  74.  72.  76.  71.\n","  88. 107. 109. 125. 131. 147. 130. 120. 127. 146. 152. 122.  51.  27.\n","  26.  99.  71.  80.  87.  81.  72.  70.  82.  84.  97. 276. 108. 100.\n","  89.  87.  76.  71.  84.  91. 122. 172.  84.  82.  87.  87.  96. 104.\n","  92.  96.  93. 179.  86.  85.  86. 101. 109. 118. 117. 110.  96.  92.\n","  84.  70.  85.  66.  75. 110. 134. 127. 158.  91.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  83.0\n","quartile_high:  115.25\n","interquartile_range:  32.25\n","Percentiles: \n","25th=83.0\n","75th=115.25\n","interquartile range: 32.25\n","Identified outliers: 28\n","Non-outlier observations: 472\n","Identified outliers: 28\n","Outliers: \n"," [175.0, 16.0, 22.0, 0.0, 291.0, 191.0, 293.0, 0.0, 32.0, 198.0, 28.0, 168.0, 26.0, 0.0, 230.0, 176.0, 181.0, 178.0, 27.0, 21.0, 192.0, 0.0, 31.0, 27.0, 26.0, 276.0, 172.0, 179.0]\n","Y_outliers: \n"," [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan 175.  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  16.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  22.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan 291.  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan 191.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan 293.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  32.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan 198.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  28.  nan  nan  nan  nan  nan  nan  nan  nan 168.  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  26.   0.  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan 230.  nan  nan  nan  nan  nan  nan  nan  nan  nan 176.\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n"," 181.  nan  nan  nan  nan  nan  nan  nan  nan  nan 178.  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  27.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  21.\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan 192.  nan  nan  nan  nan  nan  nan  nan  nan  nan   0.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  31.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  27.\n","  26.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan 276.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan 172.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan 179.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n","len pos_outliers: \n"," 28\n","len pos_zeros: \n"," 4\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","\n","[4 rows x 8 columns]\n","D2-150.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D2-150.csv\n","data \n"," [[  1.  54. 101. ... 114. 102. 115.]\n"," [  2.  94.  99. ... 102. 100. 102.]\n"," [  3. 103. 101. ...  97. 102.  97.]\n"," ...\n"," [148. 107.  94. ... 103.  93. 102.]\n"," [149.  83.  95. ... 113.  95. 112.]\n"," [150. 103. 101. ...  88. 101.  87.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n","Y \n"," [[ 54. 101.  91. ... 114. 102. 115.]\n"," [ 94.  99. 106. ... 102. 100. 102.]\n"," [103. 101. 102. ...  97. 102.  97.]\n"," ...\n"," [107.  94. 111. ... 103.  93. 102.]\n"," [ 83.  95.  70. ... 113.  95. 112.]\n"," [103. 101. 133. ...  88. 101.  87.]]\n","array([[ 54., 101.,  91., ..., 114., 102., 115.],\n","       [ 94.,  99., 106., ..., 102., 100., 102.],\n","       [103., 101., 102., ...,  97., 102.,  97.],\n","       ...,\n","       [107.,  94., 111., ..., 103.,  93., 102.],\n","       [ 83.,  95.,  70., ..., 113.,  95., 112.],\n","       [103., 101., 133., ...,  88., 101.,  87.]])\n","len           X / Y:  150 150\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","len flattened X / Y:  1500 1500\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","Y flattened \n"," [ 54. 101.  91. ...  88. 101.  87.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  94.0\n","quartile_high:  106.0\n","interquartile_range:  12.0\n","Percentiles: \n","25th=94.0\n","75th=106.0\n","interquartile range: 12.0\n","Identified outliers: 27\n","Non-outlier observations: 1473\n","Identified outliers: 27\n","Outliers: \n"," [54.0, 142.0, 72.0, 68.0, 73.0, 72.0, 71.0, 126.0, 128.0, 73.0, 73.0, 73.0, 74.0, 127.0, 130.0, 128.0, 72.0, 75.0, 148.0, 130.0, 70.0, 70.0, 70.0, 127.0, 70.0, 131.0, 133.0]\n","Y_outliers: \n"," [54. nan nan ... nan nan nan]\n","len pos_outliers: \n"," 33\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","\n","[5 rows x 8 columns]\n","D2-250.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D2-250.csv\n","data \n"," [[  1. 102. 105. ...  95. 106.  95.]\n"," [  2. 101. 105. ...  95. 106.  95.]\n"," [  3.  96. 103. ...  93. 104.  93.]\n"," ...\n"," [248. 104. 104. ...  95. 103.  94.]\n"," [249. 105.  95. ... 104.  95. 103.]\n"," [250. 117. 108. ...  76. 108.  75.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n","Y \n"," [[102. 105.  94. ...  95. 106.  95.]\n"," [101. 105.  88. ...  95. 106.  95.]\n"," [ 96. 103. 106. ...  93. 104.  93.]\n"," ...\n"," [104. 104. 103. ...  95. 103.  94.]\n"," [105.  95.  98. ... 104.  95. 103.]\n"," [117. 108. 139. ...  76. 108.  75.]]\n","array([[102., 105.,  94., ...,  95., 106.,  95.],\n","       [101., 105.,  88., ...,  95., 106.,  95.],\n","       [ 96., 103., 106., ...,  93., 104.,  93.],\n","       ...,\n","       [104., 104., 103., ...,  95., 103.,  94.],\n","       [105.,  95.,  98., ..., 104.,  95., 103.],\n","       [117., 108., 139., ...,  76., 108.,  75.]])\n","len           X / Y:  250 250\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","len flattened X / Y:  2500 2500\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","Y flattened \n"," [102. 105.  94. ...  76. 108.  75.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  95.0\n","quartile_high:  105.0\n","interquartile_range:  10.0\n","Percentiles: \n","25th=95.0\n","75th=105.0\n","interquartile range: 10.0\n","Identified outliers: 84\n","Non-outlier observations: 2416\n","Identified outliers: 84\n","Outliers: \n"," [125.0, 125.0, 125.0, 122.0, 122.0, 131.0, 124.0, 123.0, 77.0, 127.0, 128.0, 65.0, 122.0, 65.0, 136.0, 122.0, 122.0, 122.0, 79.0, 122.0, 122.0, 72.0, 79.0, 79.0, 121.0, 121.0, 74.0, 72.0, 127.0, 121.0, 77.0, 122.0, 122.0, 122.0, 121.0, 121.0, 78.0, 123.0, 121.0, 128.0, 125.0, 76.0, 76.0, 76.0, 76.0, 125.0, 71.0, 68.0, 127.0, 62.0, 70.0, 77.0, 71.0, 79.0, 77.0, 78.0, 76.0, 127.0, 79.0, 79.0, 76.0, 122.0, 136.0, 71.0, 70.0, 70.0, 122.0, 60.0, 124.0, 124.0, 124.0, 125.0, 132.0, 126.0, 69.0, 75.0, 121.0, 72.0, 123.0, 132.0, 139.0, 75.0, 76.0, 75.0]\n","Y_outliers: \n"," [nan nan nan ... 76. nan 75.]\n","len pos_outliers: \n"," 114\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","5  ../data/D2-250.csv        114  ...        80.00        120.00\n","\n","[6 rows x 8 columns]\n","D2-400.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D2-400.csv\n","data \n"," [[  1.  94.  97. ... 111.  98. 111.]\n"," [  2.  91. 103. ...  96. 104.  97.]\n"," [  3. 113.  97. ... 105.  98. 105.]\n"," ...\n"," [398. 114.  94. ... 101.  94. 100.]\n"," [399. 103. 113. ...  91. 113.  90.]\n"," [400. 112. 100. ...  93. 100.  92.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n"," 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266.\n"," 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280.\n"," 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294.\n"," 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308.\n"," 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322.\n"," 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336.\n"," 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350.\n"," 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363. 364.\n"," 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377. 378.\n"," 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391. 392.\n"," 393. 394. 395. 396. 397. 398. 399. 400.]\n","Y \n"," [[ 94.  97.  88. ... 111.  98. 111.]\n"," [ 91. 103. 102. ...  96. 104.  97.]\n"," [113.  97. 100. ... 105.  98. 105.]\n"," ...\n"," [114.  94.  88. ... 101.  94. 100.]\n"," [103. 113.  77. ...  91. 113.  90.]\n"," [112. 100. 102. ...  93. 100.  92.]]\n","array([[ 94.,  97.,  88., ..., 111.,  98., 111.],\n","       [ 91., 103., 102., ...,  96., 104.,  97.],\n","       [113.,  97., 100., ..., 105.,  98., 105.],\n","       ...,\n","       [114.,  94.,  88., ..., 101.,  94., 100.],\n","       [103., 113.,  77., ...,  91., 113.,  90.],\n","       [112., 100., 102., ...,  93., 100.,  92.]])\n","len           X / Y:  400 400\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","len flattened X / Y:  4000 4000\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","Y flattened \n"," [ 94.  97.  88. ...  93. 100.  92.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  94.0\n","quartile_high:  105.0\n","interquartile_range:  11.0\n","Percentiles: \n","25th=94.0\n","75th=105.0\n","interquartile range: 11.0\n","Identified outliers: 117\n","Non-outlier observations: 3883\n","Identified outliers: 117\n","Outliers: \n"," [76.0, 68.0, 128.0, 123.0, 123.0, 123.0, 77.0, 128.0, 75.0, 124.0, 122.0, 125.0, 123.0, 126.0, 70.0, 125.0, 71.0, 74.0, 122.0, 133.0, 130.0, 122.0, 123.0, 75.0, 137.0, 124.0, 72.0, 126.0, 141.0, 75.0, 71.0, 72.0, 127.0, 77.0, 75.0, 125.0, 73.0, 73.0, 73.0, 67.0, 74.0, 127.0, 76.0, 122.0, 70.0, 71.0, 124.0, 133.0, 130.0, 77.0, 125.0, 125.0, 125.0, 75.0, 72.0, 127.0, 124.0, 76.0, 122.0, 61.0, 72.0, 75.0, 123.0, 133.0, 130.0, 77.0, 130.0, 134.0, 73.0, 143.0, 125.0, 76.0, 126.0, 124.0, 123.0, 123.0, 123.0, 122.0, 125.0, 71.0, 125.0, 122.0, 133.0, 123.0, 74.0, 122.0, 125.0, 126.0, 129.0, 71.0, 123.0, 77.0, 124.0, 77.0, 135.0, 124.0, 125.0, 126.0, 67.0, 128.0, 75.0, 123.0, 65.0, 122.0, 77.0, 124.0, 123.0, 75.0, 63.0, 125.0, 72.0, 75.0, 75.0, 123.0, 123.0, 122.0, 77.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 117\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","5  ../data/D2-250.csv        114  ...        80.00        120.00\n","6  ../data/D2-400.csv        117  ...        77.50        121.50\n","\n","[7 rows x 8 columns]\n","D2-50.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D2-50.csv\n","data \n"," [[  1. 102. 108. 101.  96. 109.  88. 109.  89. 109.  89.]\n"," [  2. 106.  92.  86. 118.  92. 107.  92. 107.  93. 107.]\n"," [  3. 106. 101. 111.  82. 102.  98. 102.  98. 102.  98.]\n"," [  4.  75. 100.  96. 111. 101. 105. 101. 105. 101. 105.]\n"," [  5. 108. 109. 105.  92. 110.  85. 110.  85. 110.  86.]\n"," [  6.  94. 103.  90.  98. 103. 102. 103. 102. 103. 102.]\n"," [  7. 111.  92. 118.  94.  92. 103.  92. 103.  92. 103.]\n"," [  8. 107.  98. 111.  87.  99. 100.  99. 100.  99. 100.]\n"," [  9.  79. 105.  78. 102. 106. 106. 106. 106. 106. 106.]\n"," [ 10. 110. 105.  96. 106. 105.  89. 105.  89. 106.  89.]\n"," [ 11. 109.  99. 112. 107. 100.  91. 100.  91. 100.  91.]\n"," [ 12.  91. 101. 101.  98. 101. 102. 101. 102. 101. 102.]\n"," [ 13.  93.  95.  74. 118.  95. 111.  96. 111.  96. 111.]\n"," [ 14. 123. 108. 103.  84. 108.  86. 108.  86. 108.  86.]\n"," [ 15.  96. 105. 100. 106. 106.  91. 106.  92. 106.  92.]\n"," [ 16. 116.  90.  96. 107.  90. 107.  90. 107.  90. 107.]\n"," [ 17.  93. 103. 106. 101. 102.  97. 102.  97. 102.  97.]\n"," [ 18.  95.  91. 104.  87.  90. 117.  91. 117.  91. 117.]\n"," [ 19.  97. 100. 120. 104.  99.  94.  99.  94.  99.  94.]\n"," [ 20.  93. 102.  98. 101. 101. 101. 101. 101. 101. 101.]\n"," [ 21. 119. 101. 109.  98. 100.  91. 100.  91. 100.  91.]\n"," [ 22.  91.  98. 116.  78.  97. 108.  98. 108.  98. 108.]\n"," [ 23.  91. 101.  83. 100. 101. 107. 101. 107. 101. 108.]\n"," [ 24.  91. 115.  69.  92. 114.  97. 114.  97. 114.  97.]\n"," [ 25. 124.  90. 114.  90.  90. 104.  90. 104.  90. 104.]\n"," [ 26. 119.  95.  90. 111.  95. 100.  95. 100.  95. 100.]\n"," [ 27. 113.  93. 108.  95.  92. 105.  92. 105.  92. 105.]\n"," [ 28. 116. 101.  73. 112. 100.  99. 100.  99. 101.  99.]\n"," [ 29.  99.  95. 120.  99.  94. 101.  95. 101.  95. 101.]\n"," [ 30.  95.  99. 110.  84.  99. 105.  99. 105.  99. 105.]\n"," [ 31.  97. 106. 104.  73. 107. 100. 106. 100. 106. 101.]\n"," [ 32.  84.  92. 107.  98.  93. 114.  92. 114.  92. 114.]\n"," [ 33. 105. 100.  93.  86. 101. 105. 100. 105. 100. 105.]\n"," [ 34. 103. 112. 107.  92. 113.  83. 112.  83. 112.  83.]\n"," [ 35.  81. 114.  91. 125. 114.  83. 113.  83. 113.  83.]\n"," [ 36. 107. 105. 113.  88. 105.  91. 105.  90. 105.  91.]\n"," [ 37. 107. 108. 108.  88. 107.  90. 107.  89. 107.  89.]\n"," [ 38.  81.  94.  77. 132.  93. 113.  93. 112.  93. 112.]\n"," [ 39.  91. 101.  82. 118. 100. 103. 100. 102. 101. 102.]\n"," [ 40. 116. 100. 105. 106. 100.  91. 100.  91. 100.  91.]\n"," [ 41. 105. 100.  97. 111. 101.  95. 101.  95. 100.  95.]\n"," [ 42.  77.  96.  80. 119.  97. 112.  97. 113.  96. 113.]\n"," [ 43. 100.  92. 104. 117.  92. 104.  92. 104.  91. 104.]\n"," [ 44. 106.  95. 104.  87.  96. 107.  96. 107.  95. 107.]\n"," [ 45.  69. 106.  97. 100. 106. 103. 107. 103. 106. 103.]\n"," [ 46. 112. 105. 146.  86. 105.  79. 105.  79. 105.  78.]\n"," [ 47. 114.  92. 119.  89.  92. 103.  92. 104.  92. 103.]\n"," [ 48.  94.  86.  90. 116.  85. 120.  85. 120.  85. 119.]\n"," [ 49.  93.  92.  87. 122.  91. 111.  91. 111.  92. 110.]\n"," [ 50.  96. 109.  91.  89. 109.  96. 109.  96. 109.  96.]]\n","X \n"," [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n"," 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n"," 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n","Y \n"," [[102. 108. 101.  96. 109.  88. 109.  89. 109.  89.]\n"," [106.  92.  86. 118.  92. 107.  92. 107.  93. 107.]\n"," [106. 101. 111.  82. 102.  98. 102.  98. 102.  98.]\n"," [ 75. 100.  96. 111. 101. 105. 101. 105. 101. 105.]\n"," [108. 109. 105.  92. 110.  85. 110.  85. 110.  86.]\n"," [ 94. 103.  90.  98. 103. 102. 103. 102. 103. 102.]\n"," [111.  92. 118.  94.  92. 103.  92. 103.  92. 103.]\n"," [107.  98. 111.  87.  99. 100.  99. 100.  99. 100.]\n"," [ 79. 105.  78. 102. 106. 106. 106. 106. 106. 106.]\n"," [110. 105.  96. 106. 105.  89. 105.  89. 106.  89.]\n"," [109.  99. 112. 107. 100.  91. 100.  91. 100.  91.]\n"," [ 91. 101. 101.  98. 101. 102. 101. 102. 101. 102.]\n"," [ 93.  95.  74. 118.  95. 111.  96. 111.  96. 111.]\n"," [123. 108. 103.  84. 108.  86. 108.  86. 108.  86.]\n"," [ 96. 105. 100. 106. 106.  91. 106.  92. 106.  92.]\n"," [116.  90.  96. 107.  90. 107.  90. 107.  90. 107.]\n"," [ 93. 103. 106. 101. 102.  97. 102.  97. 102.  97.]\n"," [ 95.  91. 104.  87.  90. 117.  91. 117.  91. 117.]\n"," [ 97. 100. 120. 104.  99.  94.  99.  94.  99.  94.]\n"," [ 93. 102.  98. 101. 101. 101. 101. 101. 101. 101.]\n"," [119. 101. 109.  98. 100.  91. 100.  91. 100.  91.]\n"," [ 91.  98. 116.  78.  97. 108.  98. 108.  98. 108.]\n"," [ 91. 101.  83. 100. 101. 107. 101. 107. 101. 108.]\n"," [ 91. 115.  69.  92. 114.  97. 114.  97. 114.  97.]\n"," [124.  90. 114.  90.  90. 104.  90. 104.  90. 104.]\n"," [119.  95.  90. 111.  95. 100.  95. 100.  95. 100.]\n"," [113.  93. 108.  95.  92. 105.  92. 105.  92. 105.]\n"," [116. 101.  73. 112. 100.  99. 100.  99. 101.  99.]\n"," [ 99.  95. 120.  99.  94. 101.  95. 101.  95. 101.]\n"," [ 95.  99. 110.  84.  99. 105.  99. 105.  99. 105.]\n"," [ 97. 106. 104.  73. 107. 100. 106. 100. 106. 101.]\n"," [ 84.  92. 107.  98.  93. 114.  92. 114.  92. 114.]\n"," [105. 100.  93.  86. 101. 105. 100. 105. 100. 105.]\n"," [103. 112. 107.  92. 113.  83. 112.  83. 112.  83.]\n"," [ 81. 114.  91. 125. 114.  83. 113.  83. 113.  83.]\n"," [107. 105. 113.  88. 105.  91. 105.  90. 105.  91.]\n"," [107. 108. 108.  88. 107.  90. 107.  89. 107.  89.]\n"," [ 81.  94.  77. 132.  93. 113.  93. 112.  93. 112.]\n"," [ 91. 101.  82. 118. 100. 103. 100. 102. 101. 102.]\n"," [116. 100. 105. 106. 100.  91. 100.  91. 100.  91.]\n"," [105. 100.  97. 111. 101.  95. 101.  95. 100.  95.]\n"," [ 77.  96.  80. 119.  97. 112.  97. 113.  96. 113.]\n"," [100.  92. 104. 117.  92. 104.  92. 104.  91. 104.]\n"," [106.  95. 104.  87.  96. 107.  96. 107.  95. 107.]\n"," [ 69. 106.  97. 100. 106. 103. 107. 103. 106. 103.]\n"," [112. 105. 146.  86. 105.  79. 105.  79. 105.  78.]\n"," [114.  92. 119.  89.  92. 103.  92. 104.  92. 103.]\n"," [ 94.  86.  90. 116.  85. 120.  85. 120.  85. 119.]\n"," [ 93.  92.  87. 122.  91. 111.  91. 111.  92. 110.]\n"," [ 96. 109.  91.  89. 109.  96. 109.  96. 109.  96.]]\n","array([[102., 108., 101.,  96., 109.,  88., 109.,  89., 109.,  89.],\n","       [106.,  92.,  86., 118.,  92., 107.,  92., 107.,  93., 107.],\n","       [106., 101., 111.,  82., 102.,  98., 102.,  98., 102.,  98.],\n","       [ 75., 100.,  96., 111., 101., 105., 101., 105., 101., 105.],\n","       [108., 109., 105.,  92., 110.,  85., 110.,  85., 110.,  86.],\n","       [ 94., 103.,  90.,  98., 103., 102., 103., 102., 103., 102.],\n","       [111.,  92., 118.,  94.,  92., 103.,  92., 103.,  92., 103.],\n","       [107.,  98., 111.,  87.,  99., 100.,  99., 100.,  99., 100.],\n","       [ 79., 105.,  78., 102., 106., 106., 106., 106., 106., 106.],\n","       [110., 105.,  96., 106., 105.,  89., 105.,  89., 106.,  89.],\n","       [109.,  99., 112., 107., 100.,  91., 100.,  91., 100.,  91.],\n","       [ 91., 101., 101.,  98., 101., 102., 101., 102., 101., 102.],\n","       [ 93.,  95.,  74., 118.,  95., 111.,  96., 111.,  96., 111.],\n","       [123., 108., 103.,  84., 108.,  86., 108.,  86., 108.,  86.],\n","       [ 96., 105., 100., 106., 106.,  91., 106.,  92., 106.,  92.],\n","       [116.,  90.,  96., 107.,  90., 107.,  90., 107.,  90., 107.],\n","       [ 93., 103., 106., 101., 102.,  97., 102.,  97., 102.,  97.],\n","       [ 95.,  91., 104.,  87.,  90., 117.,  91., 117.,  91., 117.],\n","       [ 97., 100., 120., 104.,  99.,  94.,  99.,  94.,  99.,  94.],\n","       [ 93., 102.,  98., 101., 101., 101., 101., 101., 101., 101.],\n","       [119., 101., 109.,  98., 100.,  91., 100.,  91., 100.,  91.],\n","       [ 91.,  98., 116.,  78.,  97., 108.,  98., 108.,  98., 108.],\n","       [ 91., 101.,  83., 100., 101., 107., 101., 107., 101., 108.],\n","       [ 91., 115.,  69.,  92., 114.,  97., 114.,  97., 114.,  97.],\n","       [124.,  90., 114.,  90.,  90., 104.,  90., 104.,  90., 104.],\n","       [119.,  95.,  90., 111.,  95., 100.,  95., 100.,  95., 100.],\n","       [113.,  93., 108.,  95.,  92., 105.,  92., 105.,  92., 105.],\n","       [116., 101.,  73., 112., 100.,  99., 100.,  99., 101.,  99.],\n","       [ 99.,  95., 120.,  99.,  94., 101.,  95., 101.,  95., 101.],\n","       [ 95.,  99., 110.,  84.,  99., 105.,  99., 105.,  99., 105.],\n","       [ 97., 106., 104.,  73., 107., 100., 106., 100., 106., 101.],\n","       [ 84.,  92., 107.,  98.,  93., 114.,  92., 114.,  92., 114.],\n","       [105., 100.,  93.,  86., 101., 105., 100., 105., 100., 105.],\n","       [103., 112., 107.,  92., 113.,  83., 112.,  83., 112.,  83.],\n","       [ 81., 114.,  91., 125., 114.,  83., 113.,  83., 113.,  83.],\n","       [107., 105., 113.,  88., 105.,  91., 105.,  90., 105.,  91.],\n","       [107., 108., 108.,  88., 107.,  90., 107.,  89., 107.,  89.],\n","       [ 81.,  94.,  77., 132.,  93., 113.,  93., 112.,  93., 112.],\n","       [ 91., 101.,  82., 118., 100., 103., 100., 102., 101., 102.],\n","       [116., 100., 105., 106., 100.,  91., 100.,  91., 100.,  91.],\n","       [105., 100.,  97., 111., 101.,  95., 101.,  95., 100.,  95.],\n","       [ 77.,  96.,  80., 119.,  97., 112.,  97., 113.,  96., 113.],\n","       [100.,  92., 104., 117.,  92., 104.,  92., 104.,  91., 104.],\n","       [106.,  95., 104.,  87.,  96., 107.,  96., 107.,  95., 107.],\n","       [ 69., 106.,  97., 100., 106., 103., 107., 103., 106., 103.],\n","       [112., 105., 146.,  86., 105.,  79., 105.,  79., 105.,  78.],\n","       [114.,  92., 119.,  89.,  92., 103.,  92., 104.,  92., 103.],\n","       [ 94.,  86.,  90., 116.,  85., 120.,  85., 120.,  85., 119.],\n","       [ 93.,  92.,  87., 122.,  91., 111.,  91., 111.,  92., 110.],\n","       [ 96., 109.,  91.,  89., 109.,  96., 109.,  96., 109.,  96.]])\n","len           X / Y:  50 50\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","len flattened X / Y:  500 500\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","Y flattened \n"," [102. 108. 101.  96. 109.  88. 109.  89. 109.  89. 106.  92.  86. 118.\n","  92. 107.  92. 107.  93. 107. 106. 101. 111.  82. 102.  98. 102.  98.\n"," 102.  98.  75. 100.  96. 111. 101. 105. 101. 105. 101. 105. 108. 109.\n"," 105.  92. 110.  85. 110.  85. 110.  86.  94. 103.  90.  98. 103. 102.\n"," 103. 102. 103. 102. 111.  92. 118.  94.  92. 103.  92. 103.  92. 103.\n"," 107.  98. 111.  87.  99. 100.  99. 100.  99. 100.  79. 105.  78. 102.\n"," 106. 106. 106. 106. 106. 106. 110. 105.  96. 106. 105.  89. 105.  89.\n"," 106.  89. 109.  99. 112. 107. 100.  91. 100.  91. 100.  91.  91. 101.\n"," 101.  98. 101. 102. 101. 102. 101. 102.  93.  95.  74. 118.  95. 111.\n","  96. 111.  96. 111. 123. 108. 103.  84. 108.  86. 108.  86. 108.  86.\n","  96. 105. 100. 106. 106.  91. 106.  92. 106.  92. 116.  90.  96. 107.\n","  90. 107.  90. 107.  90. 107.  93. 103. 106. 101. 102.  97. 102.  97.\n"," 102.  97.  95.  91. 104.  87.  90. 117.  91. 117.  91. 117.  97. 100.\n"," 120. 104.  99.  94.  99.  94.  99.  94.  93. 102.  98. 101. 101. 101.\n"," 101. 101. 101. 101. 119. 101. 109.  98. 100.  91. 100.  91. 100.  91.\n","  91.  98. 116.  78.  97. 108.  98. 108.  98. 108.  91. 101.  83. 100.\n"," 101. 107. 101. 107. 101. 108.  91. 115.  69.  92. 114.  97. 114.  97.\n"," 114.  97. 124.  90. 114.  90.  90. 104.  90. 104.  90. 104. 119.  95.\n","  90. 111.  95. 100.  95. 100.  95. 100. 113.  93. 108.  95.  92. 105.\n","  92. 105.  92. 105. 116. 101.  73. 112. 100.  99. 100.  99. 101.  99.\n","  99.  95. 120.  99.  94. 101.  95. 101.  95. 101.  95.  99. 110.  84.\n","  99. 105.  99. 105.  99. 105.  97. 106. 104.  73. 107. 100. 106. 100.\n"," 106. 101.  84.  92. 107.  98.  93. 114.  92. 114.  92. 114. 105. 100.\n","  93.  86. 101. 105. 100. 105. 100. 105. 103. 112. 107.  92. 113.  83.\n"," 112.  83. 112.  83.  81. 114.  91. 125. 114.  83. 113.  83. 113.  83.\n"," 107. 105. 113.  88. 105.  91. 105.  90. 105.  91. 107. 108. 108.  88.\n"," 107.  90. 107.  89. 107.  89.  81.  94.  77. 132.  93. 113.  93. 112.\n","  93. 112.  91. 101.  82. 118. 100. 103. 100. 102. 101. 102. 116. 100.\n"," 105. 106. 100.  91. 100.  91. 100.  91. 105. 100.  97. 111. 101.  95.\n"," 101.  95. 100.  95.  77.  96.  80. 119.  97. 112.  97. 113.  96. 113.\n"," 100.  92. 104. 117.  92. 104.  92. 104.  91. 104. 106.  95. 104.  87.\n","  96. 107.  96. 107.  95. 107.  69. 106.  97. 100. 106. 103. 107. 103.\n"," 106. 103. 112. 105. 146.  86. 105.  79. 105.  79. 105.  78. 114.  92.\n"," 119.  89.  92. 103.  92. 104.  92. 103.  94.  86.  90. 116.  85. 120.\n","  85. 120.  85. 119.  93.  92.  87. 122.  91. 111.  91. 111.  92. 110.\n","  96. 109.  91.  89. 109.  96. 109.  96. 109.  96.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  92.75\n","quartile_high:  106.0\n","interquartile_range:  13.25\n","Percentiles: \n","25th=92.75\n","75th=106.0\n","interquartile range: 13.25\n","Identified outliers: 4\n","Non-outlier observations: 496\n","Identified outliers: 4\n","Outliers: \n"," [69.0, 132.0, 69.0, 146.0]\n","Y_outliers: \n"," [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  69.  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan 132.  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  69.  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan 146.  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan]\n","len pos_outliers: \n"," 4\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","5  ../data/D2-250.csv        114  ...        80.00        120.00\n","6  ../data/D2-400.csv        117  ...        77.50        121.50\n","7   ../data/D2-50.csv          4  ...        72.88        125.88\n","\n","[8 rows x 8 columns]\n","D3-150.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D3-150.csv\n","data \n"," [[  1. 111.  79. ...  99. 111. 124.]\n"," [  2. 104. 101. ... 109. 103.  90.]\n"," [  3.  99. 102. ...  99. 100.  98.]\n"," ...\n"," [148. 103.  98. ... 111. 103.  99.]\n"," [149. 105.  98. ...  95. 103.  87.]\n"," [150.  89. 113. ...  95.  89.  91.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n","Y \n"," [[111.  79. 100. ...  99. 111. 124.]\n"," [104. 101.  96. ... 109. 103.  90.]\n"," [ 99. 102. 105. ...  99. 100.  98.]\n"," ...\n"," [103.  98.  98. ... 111. 103.  99.]\n"," [105.  98. 112. ...  95. 103.  87.]\n"," [ 89. 113. 108. ...  95.  89.  91.]]\n","array([[111.,  79., 100., ...,  99., 111., 124.],\n","       [104., 101.,  96., ..., 109., 103.,  90.],\n","       [ 99., 102., 105., ...,  99., 100.,  98.],\n","       ...,\n","       [103.,  98.,  98., ..., 111., 103.,  99.],\n","       [105.,  98., 112., ...,  95., 103.,  87.],\n","       [ 89., 113., 108., ...,  95.,  89.,  91.]])\n","len           X / Y:  150 150\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","len flattened X / Y:  1500 1500\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","Y flattened \n"," [111.  79. 100. ...  95.  89.  91.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  94.0\n","quartile_high:  105.0\n","interquartile_range:  11.0\n","Percentiles: \n","25th=94.0\n","75th=105.0\n","interquartile range: 11.0\n","Identified outliers: 40\n","Non-outlier observations: 1460\n","Identified outliers: 40\n","Outliers: \n"," [124.0, 137.0, 76.0, 135.0, 122.0, 71.0, 75.0, 135.0, 139.0, 76.0, 127.0, 73.0, 123.0, 126.0, 76.0, 73.0, 77.0, 136.0, 125.0, 141.0, 71.0, 125.0, 123.0, 122.0, 74.0, 127.0, 136.0, 75.0, 76.0, 77.0, 72.0, 124.0, 69.0, 73.0, 132.0, 76.0, 128.0, 123.0, 77.0, 122.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 40\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","5  ../data/D2-250.csv        114  ...        80.00        120.00\n","6  ../data/D2-400.csv        117  ...        77.50        121.50\n","7   ../data/D2-50.csv          4  ...        72.88        125.88\n","8  ../data/D3-150.csv         40  ...        77.50        121.50\n","\n","[9 rows x 8 columns]\n","D3-250.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D3-250.csv\n","data \n"," [[  1.  92. 114. ...  92.  91.  69.]\n"," [  2. 113.  90. ... 114. 111.  80.]\n"," [  3. 101.  90. ...  98. 105. 119.]\n"," ...\n"," [248.  95. 114. ...  94.  87.  75.]\n"," [249. 115.  82. ...  92. 112.  86.]\n"," [250. 106.  95. ... 101. 106.  92.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n","Y \n"," [[ 92. 114. 105. ...  92.  91.  69.]\n"," [113.  90.  93. ... 114. 111.  80.]\n"," [101.  90.  99. ...  98. 105. 119.]\n"," ...\n"," [ 95. 114. 113. ...  94.  87.  75.]\n"," [115.  82. 104. ...  92. 112.  86.]\n"," [106.  95. 101. ... 101. 106.  92.]]\n","array([[ 92., 114., 105., ...,  92.,  91.,  69.],\n","       [113.,  90.,  93., ..., 114., 111.,  80.],\n","       [101.,  90.,  99., ...,  98., 105., 119.],\n","       ...,\n","       [ 95., 114., 113., ...,  94.,  87.,  75.],\n","       [115.,  82., 104., ...,  92., 112.,  86.],\n","       [106.,  95., 101., ..., 101., 106.,  92.]])\n","len           X / Y:  250 250\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","len flattened X / Y:  2500 2500\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","Y flattened \n"," [ 92. 114. 105. ... 101. 106.  92.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  95.0\n","quartile_high:  105.0\n","interquartile_range:  10.0\n","Percentiles: \n","25th=95.0\n","75th=105.0\n","interquartile range: 10.0\n","Identified outliers: 79\n","Non-outlier observations: 2421\n","Identified outliers: 79\n","Outliers: \n"," [124.0, 69.0, 128.0, 123.0, 131.0, 79.0, 73.0, 66.0, 76.0, 76.0, 62.0, 121.0, 122.0, 123.0, 79.0, 128.0, 123.0, 134.0, 126.0, 124.0, 75.0, 77.0, 123.0, 79.0, 122.0, 128.0, 124.0, 132.0, 126.0, 78.0, 128.0, 72.0, 74.0, 121.0, 76.0, 65.0, 76.0, 125.0, 78.0, 124.0, 75.0, 124.0, 79.0, 128.0, 72.0, 121.0, 122.0, 124.0, 78.0, 128.0, 123.0, 68.0, 123.0, 76.0, 123.0, 122.0, 130.0, 79.0, 78.0, 122.0, 121.0, 129.0, 65.0, 122.0, 73.0, 137.0, 127.0, 76.0, 124.0, 76.0, 79.0, 121.0, 58.0, 127.0, 73.0, 138.0, 122.0, 77.0, 75.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 97\n","len pos_zeros: \n"," 0\n","df_summary: \n","              filename # outliers  ... lower_cutoff  upper_cutoff\n","0  ../data/D1-150.csv         40  ...       -15.00        209.00\n","1  ../data/D1-250.csv         60  ...       -52.00        244.00\n","2  ../data/D1-400.csv        100  ...      -114.50        297.50\n","3   ../data/D1-50.csv         28  ...        34.62        163.62\n","4  ../data/D2-150.csv         33  ...        76.00        124.00\n","5  ../data/D2-250.csv        114  ...        80.00        120.00\n","6  ../data/D2-400.csv        117  ...        77.50        121.50\n","7   ../data/D2-50.csv          4  ...        72.88        125.88\n","8  ../data/D3-150.csv         40  ...        77.50        121.50\n","9  ../data/D3-250.csv         97  ...        80.00        120.00\n","\n","[10 rows x 8 columns]\n","D3-400.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D3-400.csv\n","data \n"," [[  1.  98.  93. ...  93. 102. 122.]\n"," [  2.  96. 100. ...  96.  96. 114.]\n"," [  3. 105. 109. ... 110. 103.  82.]\n"," ...\n"," [398. 108.  90. ... 116. 108. 118.]\n"," [399.  97.  96. ...  92.  97. 107.]\n"," [400. 105. 105. ... 102. 103.  67.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n"," 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266.\n"," 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280.\n"," 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294.\n"," 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308.\n"," 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322.\n"," 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336.\n"," 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350.\n"," 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363. 364.\n"," 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377. 378.\n"," 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391. 392.\n"," 393. 394. 395. 396. 397. 398. 399. 400.]\n","Y \n"," [[ 98.  93. 100. ...  93. 102. 122.]\n"," [ 96. 100. 104. ...  96.  96. 114.]\n"," [105. 109.  97. ... 110. 103.  82.]\n"," ...\n"," [108.  90.  87. ... 116. 108. 118.]\n"," [ 97.  96. 105. ...  92.  97. 107.]\n"," [105. 105. 106. ... 102. 103.  67.]]\n","array([[ 98.,  93., 100., ...,  93., 102., 122.],\n","       [ 96., 100., 104., ...,  96.,  96., 114.],\n","       [105., 109.,  97., ..., 110., 103.,  82.],\n","       ...,\n","       [108.,  90.,  87., ..., 116., 108., 118.],\n","       [ 97.,  96., 105., ...,  92.,  97., 107.],\n","       [105., 105., 106., ..., 102., 103.,  67.]])\n","len           X / Y:  400 400\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","len flattened X / Y:  4000 4000\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","Y flattened \n"," [ 98.  93. 100. ... 102. 103.  67.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  94.0\n","quartile_high:  105.0\n","interquartile_range:  11.0\n","Percentiles: \n","25th=94.0\n","75th=105.0\n","interquartile range: 11.0\n","Identified outliers: 128\n","Non-outlier observations: 3872\n","Identified outliers: 128\n","Outliers: \n"," [122.0, 155.0, 76.0, 127.0, 75.0, 77.0, 124.0, 72.0, 132.0, 75.0, 74.0, 67.0, 127.0, 123.0, 125.0, 123.0, 124.0, 70.0, 141.0, 75.0, 133.0, 138.0, 76.0, 77.0, 130.0, 76.0, 122.0, 125.0, 127.0, 122.0, 122.0, 123.0, 129.0, 74.0, 122.0, 67.0, 123.0, 77.0, 72.0, 128.0, 74.0, 77.0, 131.0, 132.0, 122.0, 123.0, 74.0, 73.0, 72.0, 70.0, 123.0, 122.0, 74.0, 128.0, 70.0, 123.0, 128.0, 122.0, 128.0, 77.0, 123.0, 125.0, 124.0, 76.0, 143.0, 123.0, 130.0, 74.0, 128.0, 77.0, 123.0, 73.0, 123.0, 126.0, 127.0, 130.0, 77.0, 139.0, 126.0, 75.0, 131.0, 74.0, 124.0, 73.0, 128.0, 124.0, 75.0, 75.0, 72.0, 76.0, 122.0, 122.0, 122.0, 77.0, 123.0, 126.0, 68.0, 127.0, 76.0, 122.0, 77.0, 76.0, 123.0, 129.0, 75.0, 131.0, 76.0, 133.0, 74.0, 125.0, 127.0, 124.0, 126.0, 77.0, 72.0, 137.0, 73.0, 75.0, 131.0, 122.0, 71.0, 76.0, 122.0, 132.0, 75.0, 122.0, 75.0, 67.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan 67.]\n","len pos_outliers: \n"," 128\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","\n","[11 rows x 8 columns]\n","D3-50.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D3-50.csv\n","data \n"," [[  1.  99.  95. 114. 106. 104.  90. 109.  85. 107.  91.]\n"," [  2. 109. 118.  98.  92. 103. 115.  99. 102.  95.  69.]\n"," [  3.  99. 106. 104. 106. 101.  99. 111.  89.  97.  88.]\n"," [  4. 102. 112. 102.  91.  89. 118.  99. 101.  94.  92.]\n"," [  5.  99. 104.  94.  89. 114.  90.  95. 100. 103. 112.]\n"," [  6.  99.  90.  96. 105. 111.  87.  90. 109.  99. 114.]\n"," [  7. 106. 106.  95.  87. 115.  91.  97. 108. 109.  86.]\n"," [  8.  98. 109.  98. 112.  89. 108. 103.  99.  93.  91.]\n"," [  9. 108.  95.  95.  90.  94. 103. 103. 101. 100. 111.]\n"," [ 10. 112.  73.  97.  95.  99.  95.  90. 107. 115. 117.]\n"," [ 11.  92. 104. 111.  95. 110.  96. 106.  94.  95.  97.]\n"," [ 12. 106.  84.  97. 117.  92. 103.  95. 104.  99. 103.]\n"," [ 13.  92. 113. 109. 112.  89. 110. 105.  96.  93.  81.]\n"," [ 14. 104. 106. 106. 101. 114.  93. 101. 101. 100.  74.]\n"," [ 15.  99. 111.  82.  69. 106. 107.  88. 127.  99. 112.]\n"," [ 16. 104. 105. 108.  76. 102. 102. 111.  97. 101.  94.]\n"," [ 17.  94. 100. 105. 120. 101.  94. 102.  86. 101.  97.]\n"," [ 18. 102.  82. 103. 105.  84. 113.  87. 105. 100. 119.]\n"," [ 19. 100. 102. 106. 110.  97.  97. 105.  97.  99.  87.]\n"," [ 20.  92. 102.  92.  99.  92. 101.  93. 104.  93. 132.]\n"," [ 21. 100.  92.  92. 104. 104.  93.  83. 120. 107. 105.]\n"," [ 22. 106.  91.  98.  95. 100.  96. 103.  99. 106. 106.]\n"," [ 23. 100. 107.  88.  98.  97. 109.  94. 103.  99. 105.]\n"," [ 24. 102.  94.  92. 113.  97. 101.  98. 105. 104.  94.]\n"," [ 25. 113.  86.  92.  84. 101. 105.  92. 111. 106. 110.]\n"," [ 26.  89. 121. 124. 128.  96.  93. 129.  65.  86.  69.]\n"," [ 27.  97.  82. 103. 101.  85. 103.  98.  99. 110. 122.]\n"," [ 28. 104. 100.  98. 112.  90. 106. 103. 104.  99.  84.]\n"," [ 29. 101.  95.  90.  72. 115.  92.  96. 107. 109. 123.]\n"," [ 30. 103. 103. 101. 109.  93. 114. 100.  99.  97.  81.]\n"," [ 31.  97. 107. 102. 107.  87. 113. 105.  94.  90.  98.]\n"," [ 32.  83. 104. 101.  95. 113.  86.  97. 101.  90. 130.]\n"," [ 33.  97. 112.  99. 102. 102.  96. 105. 103.  94.  90.]\n"," [ 34. 100. 111.  96. 106.  90. 115.  99. 109.  90.  84.]\n"," [ 35. 111.  77.  97.  92. 112.  81.  97.  98. 117. 118.]\n"," [ 36.  99.  96. 114. 122. 101.  91. 110.  77. 101.  89.]\n"," [ 37. 113.  70. 109.  84.  98.  93. 101.  94. 115. 123.]\n"," [ 38.  94.  95. 106. 121.  92.  93. 109.  83. 100. 107.]\n"," [ 39.  94. 109.  99. 100. 102.  99. 101.  97.  96. 103.]\n"," [ 40.  95. 116. 104.  96. 105. 100. 108.  93.  93.  90.]\n"," [ 41.  84.  96. 107. 110.  88. 108.  96.  97.  91. 123.]\n"," [ 42. 101. 109.  97.  82. 106.  98. 103. 105. 103.  96.]\n"," [ 43. 107. 102. 101. 106. 109.  96.  96. 101. 100.  82.]\n"," [ 44.  95. 102. 106. 114.  91. 105. 108.  87.  95.  97.]\n"," [ 45. 102. 101.  87. 100. 102. 106.  87. 117. 101.  97.]\n"," [ 46.  93. 112.  97.  97.  98. 105. 110.  93.  98.  97.]\n"," [ 47.  90. 111. 102. 102. 107.  90. 105.  97.  96. 100.]\n"," [ 48. 105.  89. 108. 104.  95. 104. 104.  94. 102.  95.]\n"," [ 49. 108.  93.  94.  85. 110.  93.  94. 107. 114. 102.]\n"," [ 50. 101. 100.  84.  82. 108. 104.  80. 129.  99. 113.]]\n","X \n"," [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n"," 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n"," 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n","Y \n"," [[ 99.  95. 114. 106. 104.  90. 109.  85. 107.  91.]\n"," [109. 118.  98.  92. 103. 115.  99. 102.  95.  69.]\n"," [ 99. 106. 104. 106. 101.  99. 111.  89.  97.  88.]\n"," [102. 112. 102.  91.  89. 118.  99. 101.  94.  92.]\n"," [ 99. 104.  94.  89. 114.  90.  95. 100. 103. 112.]\n"," [ 99.  90.  96. 105. 111.  87.  90. 109.  99. 114.]\n"," [106. 106.  95.  87. 115.  91.  97. 108. 109.  86.]\n"," [ 98. 109.  98. 112.  89. 108. 103.  99.  93.  91.]\n"," [108.  95.  95.  90.  94. 103. 103. 101. 100. 111.]\n"," [112.  73.  97.  95.  99.  95.  90. 107. 115. 117.]\n"," [ 92. 104. 111.  95. 110.  96. 106.  94.  95.  97.]\n"," [106.  84.  97. 117.  92. 103.  95. 104.  99. 103.]\n"," [ 92. 113. 109. 112.  89. 110. 105.  96.  93.  81.]\n"," [104. 106. 106. 101. 114.  93. 101. 101. 100.  74.]\n"," [ 99. 111.  82.  69. 106. 107.  88. 127.  99. 112.]\n"," [104. 105. 108.  76. 102. 102. 111.  97. 101.  94.]\n"," [ 94. 100. 105. 120. 101.  94. 102.  86. 101.  97.]\n"," [102.  82. 103. 105.  84. 113.  87. 105. 100. 119.]\n"," [100. 102. 106. 110.  97.  97. 105.  97.  99.  87.]\n"," [ 92. 102.  92.  99.  92. 101.  93. 104.  93. 132.]\n"," [100.  92.  92. 104. 104.  93.  83. 120. 107. 105.]\n"," [106.  91.  98.  95. 100.  96. 103.  99. 106. 106.]\n"," [100. 107.  88.  98.  97. 109.  94. 103.  99. 105.]\n"," [102.  94.  92. 113.  97. 101.  98. 105. 104.  94.]\n"," [113.  86.  92.  84. 101. 105.  92. 111. 106. 110.]\n"," [ 89. 121. 124. 128.  96.  93. 129.  65.  86.  69.]\n"," [ 97.  82. 103. 101.  85. 103.  98.  99. 110. 122.]\n"," [104. 100.  98. 112.  90. 106. 103. 104.  99.  84.]\n"," [101.  95.  90.  72. 115.  92.  96. 107. 109. 123.]\n"," [103. 103. 101. 109.  93. 114. 100.  99.  97.  81.]\n"," [ 97. 107. 102. 107.  87. 113. 105.  94.  90.  98.]\n"," [ 83. 104. 101.  95. 113.  86.  97. 101.  90. 130.]\n"," [ 97. 112.  99. 102. 102.  96. 105. 103.  94.  90.]\n"," [100. 111.  96. 106.  90. 115.  99. 109.  90.  84.]\n"," [111.  77.  97.  92. 112.  81.  97.  98. 117. 118.]\n"," [ 99.  96. 114. 122. 101.  91. 110.  77. 101.  89.]\n"," [113.  70. 109.  84.  98.  93. 101.  94. 115. 123.]\n"," [ 94.  95. 106. 121.  92.  93. 109.  83. 100. 107.]\n"," [ 94. 109.  99. 100. 102.  99. 101.  97.  96. 103.]\n"," [ 95. 116. 104.  96. 105. 100. 108.  93.  93.  90.]\n"," [ 84.  96. 107. 110.  88. 108.  96.  97.  91. 123.]\n"," [101. 109.  97.  82. 106.  98. 103. 105. 103.  96.]\n"," [107. 102. 101. 106. 109.  96.  96. 101. 100.  82.]\n"," [ 95. 102. 106. 114.  91. 105. 108.  87.  95.  97.]\n"," [102. 101.  87. 100. 102. 106.  87. 117. 101.  97.]\n"," [ 93. 112.  97.  97.  98. 105. 110.  93.  98.  97.]\n"," [ 90. 111. 102. 102. 107.  90. 105.  97.  96. 100.]\n"," [105.  89. 108. 104.  95. 104. 104.  94. 102.  95.]\n"," [108.  93.  94.  85. 110.  93.  94. 107. 114. 102.]\n"," [101. 100.  84.  82. 108. 104.  80. 129.  99. 113.]]\n","array([[ 99.,  95., 114., 106., 104.,  90., 109.,  85., 107.,  91.],\n","       [109., 118.,  98.,  92., 103., 115.,  99., 102.,  95.,  69.],\n","       [ 99., 106., 104., 106., 101.,  99., 111.,  89.,  97.,  88.],\n","       [102., 112., 102.,  91.,  89., 118.,  99., 101.,  94.,  92.],\n","       [ 99., 104.,  94.,  89., 114.,  90.,  95., 100., 103., 112.],\n","       [ 99.,  90.,  96., 105., 111.,  87.,  90., 109.,  99., 114.],\n","       [106., 106.,  95.,  87., 115.,  91.,  97., 108., 109.,  86.],\n","       [ 98., 109.,  98., 112.,  89., 108., 103.,  99.,  93.,  91.],\n","       [108.,  95.,  95.,  90.,  94., 103., 103., 101., 100., 111.],\n","       [112.,  73.,  97.,  95.,  99.,  95.,  90., 107., 115., 117.],\n","       [ 92., 104., 111.,  95., 110.,  96., 106.,  94.,  95.,  97.],\n","       [106.,  84.,  97., 117.,  92., 103.,  95., 104.,  99., 103.],\n","       [ 92., 113., 109., 112.,  89., 110., 105.,  96.,  93.,  81.],\n","       [104., 106., 106., 101., 114.,  93., 101., 101., 100.,  74.],\n","       [ 99., 111.,  82.,  69., 106., 107.,  88., 127.,  99., 112.],\n","       [104., 105., 108.,  76., 102., 102., 111.,  97., 101.,  94.],\n","       [ 94., 100., 105., 120., 101.,  94., 102.,  86., 101.,  97.],\n","       [102.,  82., 103., 105.,  84., 113.,  87., 105., 100., 119.],\n","       [100., 102., 106., 110.,  97.,  97., 105.,  97.,  99.,  87.],\n","       [ 92., 102.,  92.,  99.,  92., 101.,  93., 104.,  93., 132.],\n","       [100.,  92.,  92., 104., 104.,  93.,  83., 120., 107., 105.],\n","       [106.,  91.,  98.,  95., 100.,  96., 103.,  99., 106., 106.],\n","       [100., 107.,  88.,  98.,  97., 109.,  94., 103.,  99., 105.],\n","       [102.,  94.,  92., 113.,  97., 101.,  98., 105., 104.,  94.],\n","       [113.,  86.,  92.,  84., 101., 105.,  92., 111., 106., 110.],\n","       [ 89., 121., 124., 128.,  96.,  93., 129.,  65.,  86.,  69.],\n","       [ 97.,  82., 103., 101.,  85., 103.,  98.,  99., 110., 122.],\n","       [104., 100.,  98., 112.,  90., 106., 103., 104.,  99.,  84.],\n","       [101.,  95.,  90.,  72., 115.,  92.,  96., 107., 109., 123.],\n","       [103., 103., 101., 109.,  93., 114., 100.,  99.,  97.,  81.],\n","       [ 97., 107., 102., 107.,  87., 113., 105.,  94.,  90.,  98.],\n","       [ 83., 104., 101.,  95., 113.,  86.,  97., 101.,  90., 130.],\n","       [ 97., 112.,  99., 102., 102.,  96., 105., 103.,  94.,  90.],\n","       [100., 111.,  96., 106.,  90., 115.,  99., 109.,  90.,  84.],\n","       [111.,  77.,  97.,  92., 112.,  81.,  97.,  98., 117., 118.],\n","       [ 99.,  96., 114., 122., 101.,  91., 110.,  77., 101.,  89.],\n","       [113.,  70., 109.,  84.,  98.,  93., 101.,  94., 115., 123.],\n","       [ 94.,  95., 106., 121.,  92.,  93., 109.,  83., 100., 107.],\n","       [ 94., 109.,  99., 100., 102.,  99., 101.,  97.,  96., 103.],\n","       [ 95., 116., 104.,  96., 105., 100., 108.,  93.,  93.,  90.],\n","       [ 84.,  96., 107., 110.,  88., 108.,  96.,  97.,  91., 123.],\n","       [101., 109.,  97.,  82., 106.,  98., 103., 105., 103.,  96.],\n","       [107., 102., 101., 106., 109.,  96.,  96., 101., 100.,  82.],\n","       [ 95., 102., 106., 114.,  91., 105., 108.,  87.,  95.,  97.],\n","       [102., 101.,  87., 100., 102., 106.,  87., 117., 101.,  97.],\n","       [ 93., 112.,  97.,  97.,  98., 105., 110.,  93.,  98.,  97.],\n","       [ 90., 111., 102., 102., 107.,  90., 105.,  97.,  96., 100.],\n","       [105.,  89., 108., 104.,  95., 104., 104.,  94., 102.,  95.],\n","       [108.,  93.,  94.,  85., 110.,  93.,  94., 107., 114., 102.],\n","       [101., 100.,  84.,  82., 108., 104.,  80., 129.,  99., 113.]])\n","len           X / Y:  50 50\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","len flattened X / Y:  500 500\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","Y flattened \n"," [ 99.  95. 114. 106. 104.  90. 109.  85. 107.  91. 109. 118.  98.  92.\n"," 103. 115.  99. 102.  95.  69.  99. 106. 104. 106. 101.  99. 111.  89.\n","  97.  88. 102. 112. 102.  91.  89. 118.  99. 101.  94.  92.  99. 104.\n","  94.  89. 114.  90.  95. 100. 103. 112.  99.  90.  96. 105. 111.  87.\n","  90. 109.  99. 114. 106. 106.  95.  87. 115.  91.  97. 108. 109.  86.\n","  98. 109.  98. 112.  89. 108. 103.  99.  93.  91. 108.  95.  95.  90.\n","  94. 103. 103. 101. 100. 111. 112.  73.  97.  95.  99.  95.  90. 107.\n"," 115. 117.  92. 104. 111.  95. 110.  96. 106.  94.  95.  97. 106.  84.\n","  97. 117.  92. 103.  95. 104.  99. 103.  92. 113. 109. 112.  89. 110.\n"," 105.  96.  93.  81. 104. 106. 106. 101. 114.  93. 101. 101. 100.  74.\n","  99. 111.  82.  69. 106. 107.  88. 127.  99. 112. 104. 105. 108.  76.\n"," 102. 102. 111.  97. 101.  94.  94. 100. 105. 120. 101.  94. 102.  86.\n"," 101.  97. 102.  82. 103. 105.  84. 113.  87. 105. 100. 119. 100. 102.\n"," 106. 110.  97.  97. 105.  97.  99.  87.  92. 102.  92.  99.  92. 101.\n","  93. 104.  93. 132. 100.  92.  92. 104. 104.  93.  83. 120. 107. 105.\n"," 106.  91.  98.  95. 100.  96. 103.  99. 106. 106. 100. 107.  88.  98.\n","  97. 109.  94. 103.  99. 105. 102.  94.  92. 113.  97. 101.  98. 105.\n"," 104.  94. 113.  86.  92.  84. 101. 105.  92. 111. 106. 110.  89. 121.\n"," 124. 128.  96.  93. 129.  65.  86.  69.  97.  82. 103. 101.  85. 103.\n","  98.  99. 110. 122. 104. 100.  98. 112.  90. 106. 103. 104.  99.  84.\n"," 101.  95.  90.  72. 115.  92.  96. 107. 109. 123. 103. 103. 101. 109.\n","  93. 114. 100.  99.  97.  81.  97. 107. 102. 107.  87. 113. 105.  94.\n","  90.  98.  83. 104. 101.  95. 113.  86.  97. 101.  90. 130.  97. 112.\n","  99. 102. 102.  96. 105. 103.  94.  90. 100. 111.  96. 106.  90. 115.\n","  99. 109.  90.  84. 111.  77.  97.  92. 112.  81.  97.  98. 117. 118.\n","  99.  96. 114. 122. 101.  91. 110.  77. 101.  89. 113.  70. 109.  84.\n","  98.  93. 101.  94. 115. 123.  94.  95. 106. 121.  92.  93. 109.  83.\n"," 100. 107.  94. 109.  99. 100. 102.  99. 101.  97.  96. 103.  95. 116.\n"," 104.  96. 105. 100. 108.  93.  93.  90.  84.  96. 107. 110.  88. 108.\n","  96.  97.  91. 123. 101. 109.  97.  82. 106.  98. 103. 105. 103.  96.\n"," 107. 102. 101. 106. 109.  96.  96. 101. 100.  82.  95. 102. 106. 114.\n","  91. 105. 108.  87.  95.  97. 102. 101.  87. 100. 102. 106.  87. 117.\n"," 101.  97.  93. 112.  97.  97.  98. 105. 110.  93.  98.  97.  90. 111.\n"," 102. 102. 107.  90. 105.  97.  96. 100. 105.  89. 108. 104.  95. 104.\n"," 104.  94. 102.  95. 108.  93.  94.  85. 110.  93.  94. 107. 114. 102.\n"," 101. 100.  84.  82. 108. 104.  80. 129.  99. 113.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  94.0\n","quartile_high:  106.0\n","interquartile_range:  12.0\n","Percentiles: \n","25th=94.0\n","75th=106.0\n","interquartile range: 12.0\n","Identified outliers: 14\n","Non-outlier observations: 486\n","Identified outliers: 14\n","Outliers: \n"," [69.0, 73.0, 74.0, 69.0, 127.0, 132.0, 128.0, 129.0, 65.0, 69.0, 72.0, 130.0, 70.0, 129.0]\n","Y_outliers: \n"," [ nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  69.  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  73.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  74.\n","  nan  nan  nan  69.  nan  nan  nan 127.  nan  nan  nan  nan  nan  76.\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan 132.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n"," 124. 128.  nan  nan 129.  65.  nan  69.  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  72.  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan 130.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  70.  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan  nan\n","  nan  nan  nan  nan  nan  nan  nan 129.  nan  nan]\n","len pos_outliers: \n"," 16\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","11   ../data/D3-50.csv         16  ...        76.00        124.00\n","\n","[12 rows x 8 columns]\n","D4-150.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D4-150.csv\n","data \n"," [[  1. 113.  93. ...  94.  72.  82.]\n"," [  2.  92. 114. ... 104.  60.  94.]\n"," [  3. 106.  93. ...  92.  72.  80.]\n"," ...\n"," [148. 100. 122. ...  85.  77.  95.]\n"," [149. 115. 103. ...  81.  81.  80.]\n"," [150. 104. 114. ...  96.  70.  91.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150.]\n","Y \n"," [[113.  93. 115. ...  94.  72.  82.]\n"," [ 92. 114. 102. ... 104.  60.  94.]\n"," [106.  93. 123. ...  92.  72.  80.]\n"," ...\n"," [100. 122.  91. ...  85.  77.  95.]\n"," [115. 103. 110. ...  81.  81.  80.]\n"," [104. 114.  94. ...  96.  70.  91.]]\n","array([[113.,  93., 115., ...,  94.,  72.,  82.],\n","       [ 92., 114., 102., ..., 104.,  60.,  94.],\n","       [106.,  93., 123., ...,  92.,  72.,  80.],\n","       ...,\n","       [100., 122.,  91., ...,  85.,  77.,  95.],\n","       [115., 103., 110., ...,  81.,  81.,  80.],\n","       [104., 114.,  94., ...,  96.,  70.,  91.]])\n","len           X / Y:  150 150\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","len flattened X / Y:  1500 1500\n","X flattened \n"," [  1.   1.   1. ... 150. 150. 150.]\n","Y flattened \n"," [113.  93. 115. ...  96.  70.  91.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  89.0\n","quartile_high:  113.0\n","interquartile_range:  24.0\n","Percentiles: \n","25th=89.0\n","75th=113.0\n","interquartile range: 24.0\n","Identified outliers: 0\n","Non-outlier observations: 1500\n","Identified outliers: 0\n","Outliers: \n"," []\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 0\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","11   ../data/D3-50.csv         16  ...        76.00        124.00\n","12  ../data/D4-150.csv          0  ...        53.00        149.00\n","\n","[13 rows x 8 columns]\n","D4-250.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D4-250.csv\n","data \n"," [[  1. 106. 112. ... 106.  98.  95.]\n"," [  2. 100. 118. ... 109.  97.  96.]\n"," [  3. 107.  99. ... 103. 103.  89.]\n"," ...\n"," [248.  99. 114. ... 110.  91. 111.]\n"," [249. 110. 108. ... 108.  93. 100.]\n"," [250.  97. 121. ... 115.  89. 104.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250.]\n","Y \n"," [[106. 112. 100. ... 106.  98.  95.]\n"," [100. 118.  92. ... 109.  97.  96.]\n"," [107.  99. 111. ... 103. 103.  89.]\n"," ...\n"," [ 99. 114.  93. ... 110.  91. 111.]\n"," [110. 108.  99. ... 108.  93. 100.]\n"," [ 97. 121.  91. ... 115.  89. 104.]]\n","array([[106., 112., 100., ..., 106.,  98.,  95.],\n","       [100., 118.,  92., ..., 109.,  97.,  96.],\n","       [107.,  99., 111., ..., 103., 103.,  89.],\n","       ...,\n","       [ 99., 114.,  93., ..., 110.,  91., 111.],\n","       [110., 108.,  99., ..., 108.,  93., 100.],\n","       [ 97., 121.,  91., ..., 115.,  89., 104.]])\n","len           X / Y:  250 250\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","len flattened X / Y:  2500 2500\n","X flattened \n"," [  1.   1.   1. ... 250. 250. 250.]\n","Y flattened \n"," [106. 112. 100. ... 115.  89. 104.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  93.0\n","quartile_high:  108.0\n","interquartile_range:  15.0\n","Percentiles: \n","25th=93.0\n","75th=108.0\n","interquartile range: 15.0\n","Identified outliers: 50\n","Non-outlier observations: 2450\n","Identified outliers: 50\n","Outliers: \n"," [63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0, 63.0, 63.0, 58.0, 68.0, 66.0]\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 50\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","11   ../data/D3-50.csv         16  ...        76.00        124.00\n","12  ../data/D4-150.csv          0  ...        53.00        149.00\n","13  ../data/D4-250.csv         50  ...        70.50        130.50\n","\n","[14 rows x 8 columns]\n","D4-400.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D4-400.csv\n","data \n"," [[  1.  82. 141. ...  97.  93.  77.]\n"," [  2.  82. 141. ...  97.  93.  77.]\n"," [  3.  85. 137. ... 100.  88.  76.]\n"," ...\n"," [398.  91. 138. ...  87.  94.  80.]\n"," [399.  89. 134. ...  94.  94.  82.]\n"," [400.  89. 134. ...  94.  94.  82.]]\n","X \n"," [  1.   2.   3.   4.   5.   6.   7.   8.   9.  10.  11.  12.  13.  14.\n","  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.  28.\n","  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.  42.\n","  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.  56.\n","  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.  70.\n","  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.  84.\n","  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.  98.\n","  99. 100. 101. 102. 103. 104. 105. 106. 107. 108. 109. 110. 111. 112.\n"," 113. 114. 115. 116. 117. 118. 119. 120. 121. 122. 123. 124. 125. 126.\n"," 127. 128. 129. 130. 131. 132. 133. 134. 135. 136. 137. 138. 139. 140.\n"," 141. 142. 143. 144. 145. 146. 147. 148. 149. 150. 151. 152. 153. 154.\n"," 155. 156. 157. 158. 159. 160. 161. 162. 163. 164. 165. 166. 167. 168.\n"," 169. 170. 171. 172. 173. 174. 175. 176. 177. 178. 179. 180. 181. 182.\n"," 183. 184. 185. 186. 187. 188. 189. 190. 191. 192. 193. 194. 195. 196.\n"," 197. 198. 199. 200. 201. 202. 203. 204. 205. 206. 207. 208. 209. 210.\n"," 211. 212. 213. 214. 215. 216. 217. 218. 219. 220. 221. 222. 223. 224.\n"," 225. 226. 227. 228. 229. 230. 231. 232. 233. 234. 235. 236. 237. 238.\n"," 239. 240. 241. 242. 243. 244. 245. 246. 247. 248. 249. 250. 251. 252.\n"," 253. 254. 255. 256. 257. 258. 259. 260. 261. 262. 263. 264. 265. 266.\n"," 267. 268. 269. 270. 271. 272. 273. 274. 275. 276. 277. 278. 279. 280.\n"," 281. 282. 283. 284. 285. 286. 287. 288. 289. 290. 291. 292. 293. 294.\n"," 295. 296. 297. 298. 299. 300. 301. 302. 303. 304. 305. 306. 307. 308.\n"," 309. 310. 311. 312. 313. 314. 315. 316. 317. 318. 319. 320. 321. 322.\n"," 323. 324. 325. 326. 327. 328. 329. 330. 331. 332. 333. 334. 335. 336.\n"," 337. 338. 339. 340. 341. 342. 343. 344. 345. 346. 347. 348. 349. 350.\n"," 351. 352. 353. 354. 355. 356. 357. 358. 359. 360. 361. 362. 363. 364.\n"," 365. 366. 367. 368. 369. 370. 371. 372. 373. 374. 375. 376. 377. 378.\n"," 379. 380. 381. 382. 383. 384. 385. 386. 387. 388. 389. 390. 391. 392.\n"," 393. 394. 395. 396. 397. 398. 399. 400.]\n","Y \n"," [[ 82. 141. 118. ...  97.  93.  77.]\n"," [ 82. 141. 118. ...  97.  93.  77.]\n"," [ 85. 137. 120. ... 100.  88.  76.]\n"," ...\n"," [ 91. 138. 116. ...  87.  94.  80.]\n"," [ 89. 134. 117. ...  94.  94.  82.]\n"," [ 89. 134. 117. ...  94.  94.  82.]]\n","array([[ 82., 141., 118., ...,  97.,  93.,  77.],\n","       [ 82., 141., 118., ...,  97.,  93.,  77.],\n","       [ 85., 137., 120., ..., 100.,  88.,  76.],\n","       ...,\n","       [ 91., 138., 116., ...,  87.,  94.,  80.],\n","       [ 89., 134., 117., ...,  94.,  94.,  82.],\n","       [ 89., 134., 117., ...,  94.,  94.,  82.]])\n","len           X / Y:  400 400\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","len flattened X / Y:  4000 4000\n","X flattened \n"," [  1.   1.   1. ... 400. 400. 400.]\n","Y flattened \n"," [ 82. 141. 118. ...  94.  94.  82.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  84.75\n","quartile_high:  112.25\n","interquartile_range:  27.5\n","Percentiles: \n","25th=84.75\n","75th=112.25\n","interquartile range: 27.5\n","Identified outliers: 0\n","Non-outlier observations: 4000\n","Identified outliers: 0\n","Outliers: \n"," []\n","Y_outliers: \n"," [nan nan nan ... nan nan nan]\n","len pos_outliers: \n"," 0\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","11   ../data/D3-50.csv         16  ...        76.00        124.00\n","12  ../data/D4-150.csv          0  ...        53.00        149.00\n","13  ../data/D4-250.csv         50  ...        70.50        130.50\n","14  ../data/D4-400.csv          0  ...        43.50        153.50\n","\n","[15 rows x 8 columns]\n","D4-50.csv\n","file: /content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/../data/D4-50.csv\n","data \n"," [[  1. 109.  99. 126.  92.  75.  79. 130.  83. 102. 105.]\n"," [  2.  76. 132.  81. 137.  55.  99.  93. 120.  64. 143.]\n"," [  3. 133.  70. 143.  88. 104.  53. 139.  68. 116.  86.]\n"," [  4.  98. 105.  94. 137.  75.  82.  94. 113.  89. 113.]\n"," [  5. 146.  68. 131.  94. 118.  49. 127.  82. 120.  65.]\n"," [  6. 105. 109.  99. 126.  92.  75.  79. 130.  83. 102.]\n"," [  7. 143.  76. 132.  81. 137.  55.  99.  93. 120.  64.]\n"," [  8.  86. 133.  70. 143.  88. 104.  53. 139.  68. 116.]\n"," [  9. 113.  98. 105.  94. 137.  75.  82.  94. 113.  89.]\n"," [ 10.  65. 146.  68. 131.  94. 118.  49. 127.  82. 120.]\n"," [ 11. 102. 105. 109.  99. 126.  92.  75.  79. 130.  83.]\n"," [ 12.  64. 143.  76. 132.  81. 137.  55.  99.  93. 120.]\n"," [ 13. 116.  86. 133.  70. 143.  88. 104.  53. 139.  68.]\n"," [ 14.  89. 113.  98. 105.  94. 137.  75.  82.  94. 113.]\n"," [ 15. 120.  65. 146.  68. 131.  94. 118.  49. 127.  82.]\n"," [ 16.  83. 102. 105. 109.  99. 126.  92.  75.  79. 130.]\n"," [ 17. 120.  64. 143.  76. 132.  81. 137.  55.  99.  93.]\n"," [ 18.  68. 116.  86. 133.  70. 143.  88. 104.  53. 139.]\n"," [ 19. 113.  89. 113.  98. 105.  94. 137.  75.  82.  94.]\n"," [ 20.  82. 120.  65. 146.  68. 131.  94. 118.  49. 127.]\n"," [ 21. 130.  83. 102. 105. 109.  99. 126.  92.  75.  79.]\n"," [ 22.  93. 120.  64. 143.  76. 132.  81. 137.  55.  99.]\n"," [ 23. 139.  68. 116.  86. 133.  70. 143.  88. 104.  53.]\n"," [ 24.  94. 113.  89. 113.  98. 105.  94. 137.  75.  82.]\n"," [ 25. 127.  82. 120.  65. 146.  68. 131.  94. 118.  49.]\n"," [ 26.  79. 130.  83. 102. 105. 109.  99. 126.  92.  75.]\n"," [ 27.  99.  93. 120.  64. 143.  76. 132.  81. 137.  55.]\n"," [ 28.  53. 139.  68. 116.  86. 133.  70. 143.  88. 104.]\n"," [ 29.  82.  94. 113.  89. 113.  98. 105.  94. 137.  75.]\n"," [ 30.  49. 127.  82. 120.  65. 146.  68. 131.  94. 118.]\n"," [ 31.  75.  79. 130.  83. 102. 105. 109.  99. 126.  92.]\n"," [ 32.  55.  99.  93. 120.  64. 143.  76. 132.  81. 137.]\n"," [ 33. 104.  53. 139.  68. 116.  86. 133.  70. 143.  88.]\n"," [ 34.  75.  82.  94. 113.  89. 113.  98. 105.  94. 137.]\n"," [ 35. 118.  49. 127.  82. 120.  65. 146.  68. 131.  94.]\n"," [ 36.  92.  75.  79. 130.  83. 102. 105. 109.  99. 126.]\n"," [ 37. 137.  55.  99.  93. 120.  64. 143.  76. 132.  81.]\n"," [ 38.  88. 104.  53. 139.  68. 116.  86. 133.  70. 143.]\n"," [ 39. 137.  75.  82.  94. 113.  89. 113.  98. 105.  94.]\n"," [ 40.  94. 118.  49. 127.  82. 120.  65. 146.  68. 131.]\n"," [ 41. 126.  92.  75.  79. 130.  83. 102. 105. 109.  99.]\n"," [ 42.  81. 137.  55.  99.  93. 120.  64. 143.  76. 132.]\n"," [ 43. 143.  88. 104.  53. 139.  68. 116.  86. 133.  70.]\n"," [ 44.  94. 137.  75.  82.  94. 113.  89. 113.  98. 105.]\n"," [ 45. 131.  94. 118.  49. 127.  82. 120.  65. 146.  68.]\n"," [ 46.  99. 126.  92.  75.  79. 130.  83. 102. 105. 109.]\n"," [ 47. 132.  81. 137.  55.  99.  93. 120.  64. 143.  76.]\n"," [ 48.  70. 143.  88. 104.  53. 139.  68. 116.  86. 133.]\n"," [ 49. 105.  94. 137.  75.  82.  94. 113.  89. 113.  98.]\n"," [ 50.  68. 131.  94. 118.  49. 127.  82. 120.  65. 146.]]\n","X \n"," [ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n"," 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n"," 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50.]\n","Y \n"," [[109.  99. 126.  92.  75.  79. 130.  83. 102. 105.]\n"," [ 76. 132.  81. 137.  55.  99.  93. 120.  64. 143.]\n"," [133.  70. 143.  88. 104.  53. 139.  68. 116.  86.]\n"," [ 98. 105.  94. 137.  75.  82.  94. 113.  89. 113.]\n"," [146.  68. 131.  94. 118.  49. 127.  82. 120.  65.]\n"," [105. 109.  99. 126.  92.  75.  79. 130.  83. 102.]\n"," [143.  76. 132.  81. 137.  55.  99.  93. 120.  64.]\n"," [ 86. 133.  70. 143.  88. 104.  53. 139.  68. 116.]\n"," [113.  98. 105.  94. 137.  75.  82.  94. 113.  89.]\n"," [ 65. 146.  68. 131.  94. 118.  49. 127.  82. 120.]\n"," [102. 105. 109.  99. 126.  92.  75.  79. 130.  83.]\n"," [ 64. 143.  76. 132.  81. 137.  55.  99.  93. 120.]\n"," [116.  86. 133.  70. 143.  88. 104.  53. 139.  68.]\n"," [ 89. 113.  98. 105.  94. 137.  75.  82.  94. 113.]\n"," [120.  65. 146.  68. 131.  94. 118.  49. 127.  82.]\n"," [ 83. 102. 105. 109.  99. 126.  92.  75.  79. 130.]\n"," [120.  64. 143.  76. 132.  81. 137.  55.  99.  93.]\n"," [ 68. 116.  86. 133.  70. 143.  88. 104.  53. 139.]\n"," [113.  89. 113.  98. 105.  94. 137.  75.  82.  94.]\n"," [ 82. 120.  65. 146.  68. 131.  94. 118.  49. 127.]\n"," [130.  83. 102. 105. 109.  99. 126.  92.  75.  79.]\n"," [ 93. 120.  64. 143.  76. 132.  81. 137.  55.  99.]\n"," [139.  68. 116.  86. 133.  70. 143.  88. 104.  53.]\n"," [ 94. 113.  89. 113.  98. 105.  94. 137.  75.  82.]\n"," [127.  82. 120.  65. 146.  68. 131.  94. 118.  49.]\n"," [ 79. 130.  83. 102. 105. 109.  99. 126.  92.  75.]\n"," [ 99.  93. 120.  64. 143.  76. 132.  81. 137.  55.]\n"," [ 53. 139.  68. 116.  86. 133.  70. 143.  88. 104.]\n"," [ 82.  94. 113.  89. 113.  98. 105.  94. 137.  75.]\n"," [ 49. 127.  82. 120.  65. 146.  68. 131.  94. 118.]\n"," [ 75.  79. 130.  83. 102. 105. 109.  99. 126.  92.]\n"," [ 55.  99.  93. 120.  64. 143.  76. 132.  81. 137.]\n"," [104.  53. 139.  68. 116.  86. 133.  70. 143.  88.]\n"," [ 75.  82.  94. 113.  89. 113.  98. 105.  94. 137.]\n"," [118.  49. 127.  82. 120.  65. 146.  68. 131.  94.]\n"," [ 92.  75.  79. 130.  83. 102. 105. 109.  99. 126.]\n"," [137.  55.  99.  93. 120.  64. 143.  76. 132.  81.]\n"," [ 88. 104.  53. 139.  68. 116.  86. 133.  70. 143.]\n"," [137.  75.  82.  94. 113.  89. 113.  98. 105.  94.]\n"," [ 94. 118.  49. 127.  82. 120.  65. 146.  68. 131.]\n"," [126.  92.  75.  79. 130.  83. 102. 105. 109.  99.]\n"," [ 81. 137.  55.  99.  93. 120.  64. 143.  76. 132.]\n"," [143.  88. 104.  53. 139.  68. 116.  86. 133.  70.]\n"," [ 94. 137.  75.  82.  94. 113.  89. 113.  98. 105.]\n"," [131.  94. 118.  49. 127.  82. 120.  65. 146.  68.]\n"," [ 99. 126.  92.  75.  79. 130.  83. 102. 105. 109.]\n"," [132.  81. 137.  55.  99.  93. 120.  64. 143.  76.]\n"," [ 70. 143.  88. 104.  53. 139.  68. 116.  86. 133.]\n"," [105.  94. 137.  75.  82.  94. 113.  89. 113.  98.]\n"," [ 68. 131.  94. 118.  49. 127.  82. 120.  65. 146.]]\n","array([[109.,  99., 126.,  92.,  75.,  79., 130.,  83., 102., 105.],\n","       [ 76., 132.,  81., 137.,  55.,  99.,  93., 120.,  64., 143.],\n","       [133.,  70., 143.,  88., 104.,  53., 139.,  68., 116.,  86.],\n","       [ 98., 105.,  94., 137.,  75.,  82.,  94., 113.,  89., 113.],\n","       [146.,  68., 131.,  94., 118.,  49., 127.,  82., 120.,  65.],\n","       [105., 109.,  99., 126.,  92.,  75.,  79., 130.,  83., 102.],\n","       [143.,  76., 132.,  81., 137.,  55.,  99.,  93., 120.,  64.],\n","       [ 86., 133.,  70., 143.,  88., 104.,  53., 139.,  68., 116.],\n","       [113.,  98., 105.,  94., 137.,  75.,  82.,  94., 113.,  89.],\n","       [ 65., 146.,  68., 131.,  94., 118.,  49., 127.,  82., 120.],\n","       [102., 105., 109.,  99., 126.,  92.,  75.,  79., 130.,  83.],\n","       [ 64., 143.,  76., 132.,  81., 137.,  55.,  99.,  93., 120.],\n","       [116.,  86., 133.,  70., 143.,  88., 104.,  53., 139.,  68.],\n","       [ 89., 113.,  98., 105.,  94., 137.,  75.,  82.,  94., 113.],\n","       [120.,  65., 146.,  68., 131.,  94., 118.,  49., 127.,  82.],\n","       [ 83., 102., 105., 109.,  99., 126.,  92.,  75.,  79., 130.],\n","       [120.,  64., 143.,  76., 132.,  81., 137.,  55.,  99.,  93.],\n","       [ 68., 116.,  86., 133.,  70., 143.,  88., 104.,  53., 139.],\n","       [113.,  89., 113.,  98., 105.,  94., 137.,  75.,  82.,  94.],\n","       [ 82., 120.,  65., 146.,  68., 131.,  94., 118.,  49., 127.],\n","       [130.,  83., 102., 105., 109.,  99., 126.,  92.,  75.,  79.],\n","       [ 93., 120.,  64., 143.,  76., 132.,  81., 137.,  55.,  99.],\n","       [139.,  68., 116.,  86., 133.,  70., 143.,  88., 104.,  53.],\n","       [ 94., 113.,  89., 113.,  98., 105.,  94., 137.,  75.,  82.],\n","       [127.,  82., 120.,  65., 146.,  68., 131.,  94., 118.,  49.],\n","       [ 79., 130.,  83., 102., 105., 109.,  99., 126.,  92.,  75.],\n","       [ 99.,  93., 120.,  64., 143.,  76., 132.,  81., 137.,  55.],\n","       [ 53., 139.,  68., 116.,  86., 133.,  70., 143.,  88., 104.],\n","       [ 82.,  94., 113.,  89., 113.,  98., 105.,  94., 137.,  75.],\n","       [ 49., 127.,  82., 120.,  65., 146.,  68., 131.,  94., 118.],\n","       [ 75.,  79., 130.,  83., 102., 105., 109.,  99., 126.,  92.],\n","       [ 55.,  99.,  93., 120.,  64., 143.,  76., 132.,  81., 137.],\n","       [104.,  53., 139.,  68., 116.,  86., 133.,  70., 143.,  88.],\n","       [ 75.,  82.,  94., 113.,  89., 113.,  98., 105.,  94., 137.],\n","       [118.,  49., 127.,  82., 120.,  65., 146.,  68., 131.,  94.],\n","       [ 92.,  75.,  79., 130.,  83., 102., 105., 109.,  99., 126.],\n","       [137.,  55.,  99.,  93., 120.,  64., 143.,  76., 132.,  81.],\n","       [ 88., 104.,  53., 139.,  68., 116.,  86., 133.,  70., 143.],\n","       [137.,  75.,  82.,  94., 113.,  89., 113.,  98., 105.,  94.],\n","       [ 94., 118.,  49., 127.,  82., 120.,  65., 146.,  68., 131.],\n","       [126.,  92.,  75.,  79., 130.,  83., 102., 105., 109.,  99.],\n","       [ 81., 137.,  55.,  99.,  93., 120.,  64., 143.,  76., 132.],\n","       [143.,  88., 104.,  53., 139.,  68., 116.,  86., 133.,  70.],\n","       [ 94., 137.,  75.,  82.,  94., 113.,  89., 113.,  98., 105.],\n","       [131.,  94., 118.,  49., 127.,  82., 120.,  65., 146.,  68.],\n","       [ 99., 126.,  92.,  75.,  79., 130.,  83., 102., 105., 109.],\n","       [132.,  81., 137.,  55.,  99.,  93., 120.,  64., 143.,  76.],\n","       [ 70., 143.,  88., 104.,  53., 139.,  68., 116.,  86., 133.],\n","       [105.,  94., 137.,  75.,  82.,  94., 113.,  89., 113.,  98.],\n","       [ 68., 131.,  94., 118.,  49., 127.,  82., 120.,  65., 146.]])\n","len           X / Y:  50 50\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","len flattened X / Y:  500 500\n","X flattened \n"," [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  2.  2.  2.  2.  2.  2.  2.  2.\n","  2.  2.  3.  3.  3.  3.  3.  3.  3.  3.  3.  3.  4.  4.  4.  4.  4.  4.\n","  4.  4.  4.  4.  5.  5.  5.  5.  5.  5.  5.  5.  5.  5.  6.  6.  6.  6.\n","  6.  6.  6.  6.  6.  6.  7.  7.  7.  7.  7.  7.  7.  7.  7.  7.  8.  8.\n","  8.  8.  8.  8.  8.  8.  8.  8.  9.  9.  9.  9.  9.  9.  9.  9.  9.  9.\n"," 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 11. 11. 11. 11. 11. 11. 11. 11.\n"," 11. 11. 12. 12. 12. 12. 12. 12. 12. 12. 12. 12. 13. 13. 13. 13. 13. 13.\n"," 13. 13. 13. 13. 14. 14. 14. 14. 14. 14. 14. 14. 14. 14. 15. 15. 15. 15.\n"," 15. 15. 15. 15. 15. 15. 16. 16. 16. 16. 16. 16. 16. 16. 16. 16. 17. 17.\n"," 17. 17. 17. 17. 17. 17. 17. 17. 18. 18. 18. 18. 18. 18. 18. 18. 18. 18.\n"," 19. 19. 19. 19. 19. 19. 19. 19. 19. 19. 20. 20. 20. 20. 20. 20. 20. 20.\n"," 20. 20. 21. 21. 21. 21. 21. 21. 21. 21. 21. 21. 22. 22. 22. 22. 22. 22.\n"," 22. 22. 22. 22. 23. 23. 23. 23. 23. 23. 23. 23. 23. 23. 24. 24. 24. 24.\n"," 24. 24. 24. 24. 24. 24. 25. 25. 25. 25. 25. 25. 25. 25. 25. 25. 26. 26.\n"," 26. 26. 26. 26. 26. 26. 26. 26. 27. 27. 27. 27. 27. 27. 27. 27. 27. 27.\n"," 28. 28. 28. 28. 28. 28. 28. 28. 28. 28. 29. 29. 29. 29. 29. 29. 29. 29.\n"," 29. 29. 30. 30. 30. 30. 30. 30. 30. 30. 30. 30. 31. 31. 31. 31. 31. 31.\n"," 31. 31. 31. 31. 32. 32. 32. 32. 32. 32. 32. 32. 32. 32. 33. 33. 33. 33.\n"," 33. 33. 33. 33. 33. 33. 34. 34. 34. 34. 34. 34. 34. 34. 34. 34. 35. 35.\n"," 35. 35. 35. 35. 35. 35. 35. 35. 36. 36. 36. 36. 36. 36. 36. 36. 36. 36.\n"," 37. 37. 37. 37. 37. 37. 37. 37. 37. 37. 38. 38. 38. 38. 38. 38. 38. 38.\n"," 38. 38. 39. 39. 39. 39. 39. 39. 39. 39. 39. 39. 40. 40. 40. 40. 40. 40.\n"," 40. 40. 40. 40. 41. 41. 41. 41. 41. 41. 41. 41. 41. 41. 42. 42. 42. 42.\n"," 42. 42. 42. 42. 42. 42. 43. 43. 43. 43. 43. 43. 43. 43. 43. 43. 44. 44.\n"," 44. 44. 44. 44. 44. 44. 44. 44. 45. 45. 45. 45. 45. 45. 45. 45. 45. 45.\n"," 46. 46. 46. 46. 46. 46. 46. 46. 46. 46. 47. 47. 47. 47. 47. 47. 47. 47.\n"," 47. 47. 48. 48. 48. 48. 48. 48. 48. 48. 48. 48. 49. 49. 49. 49. 49. 49.\n"," 49. 49. 49. 49. 50. 50. 50. 50. 50. 50. 50. 50. 50. 50.]\n","Y flattened \n"," [109.  99. 126.  92.  75.  79. 130.  83. 102. 105.  76. 132.  81. 137.\n","  55.  99.  93. 120.  64. 143. 133.  70. 143.  88. 104.  53. 139.  68.\n"," 116.  86.  98. 105.  94. 137.  75.  82.  94. 113.  89. 113. 146.  68.\n"," 131.  94. 118.  49. 127.  82. 120.  65. 105. 109.  99. 126.  92.  75.\n","  79. 130.  83. 102. 143.  76. 132.  81. 137.  55.  99.  93. 120.  64.\n","  86. 133.  70. 143.  88. 104.  53. 139.  68. 116. 113.  98. 105.  94.\n"," 137.  75.  82.  94. 113.  89.  65. 146.  68. 131.  94. 118.  49. 127.\n","  82. 120. 102. 105. 109.  99. 126.  92.  75.  79. 130.  83.  64. 143.\n","  76. 132.  81. 137.  55.  99.  93. 120. 116.  86. 133.  70. 143.  88.\n"," 104.  53. 139.  68.  89. 113.  98. 105.  94. 137.  75.  82.  94. 113.\n"," 120.  65. 146.  68. 131.  94. 118.  49. 127.  82.  83. 102. 105. 109.\n","  99. 126.  92.  75.  79. 130. 120.  64. 143.  76. 132.  81. 137.  55.\n","  99.  93.  68. 116.  86. 133.  70. 143.  88. 104.  53. 139. 113.  89.\n"," 113.  98. 105.  94. 137.  75.  82.  94.  82. 120.  65. 146.  68. 131.\n","  94. 118.  49. 127. 130.  83. 102. 105. 109.  99. 126.  92.  75.  79.\n","  93. 120.  64. 143.  76. 132.  81. 137.  55.  99. 139.  68. 116.  86.\n"," 133.  70. 143.  88. 104.  53.  94. 113.  89. 113.  98. 105.  94. 137.\n","  75.  82. 127.  82. 120.  65. 146.  68. 131.  94. 118.  49.  79. 130.\n","  83. 102. 105. 109.  99. 126.  92.  75.  99.  93. 120.  64. 143.  76.\n"," 132.  81. 137.  55.  53. 139.  68. 116.  86. 133.  70. 143.  88. 104.\n","  82.  94. 113.  89. 113.  98. 105.  94. 137.  75.  49. 127.  82. 120.\n","  65. 146.  68. 131.  94. 118.  75.  79. 130.  83. 102. 105. 109.  99.\n"," 126.  92.  55.  99.  93. 120.  64. 143.  76. 132.  81. 137. 104.  53.\n"," 139.  68. 116.  86. 133.  70. 143.  88.  75.  82.  94. 113.  89. 113.\n","  98. 105.  94. 137. 118.  49. 127.  82. 120.  65. 146.  68. 131.  94.\n","  92.  75.  79. 130.  83. 102. 105. 109.  99. 126. 137.  55.  99.  93.\n"," 120.  64. 143.  76. 132.  81.  88. 104.  53. 139.  68. 116.  86. 133.\n","  70. 143. 137.  75.  82.  94. 113.  89. 113.  98. 105.  94.  94. 118.\n","  49. 127.  82. 120.  65. 146.  68. 131. 126.  92.  75.  79. 130.  83.\n"," 102. 105. 109.  99.  81. 137.  55.  99.  93. 120.  64. 143.  76. 132.\n"," 143.  88. 104.  53. 139.  68. 116.  86. 133.  70.  94. 137.  75.  82.\n","  94. 113.  89. 113.  98. 105. 131.  94. 118.  49. 127.  82. 120.  65.\n"," 146.  68.  99. 126.  92.  75.  79. 130.  83. 102. 105. 109. 132.  81.\n"," 137.  55.  99.  93. 120.  64. 143.  76.  70. 143.  88. 104.  53. 139.\n","  68. 116.  86. 133. 105.  94. 137.  75.  82.  94. 113.  89. 113.  98.\n","  68. 131.  94. 118.  49. 127.  82. 120.  65. 146.]\n","q_low_percentile:  25\n","q_high:  75\n","quartile_low:  81.0\n","quartile_high:  120.0\n","interquartile_range:  39.0\n","Percentiles: \n","25th=81.0\n","75th=120.0\n","interquartile range: 39.0\n","Identified outliers: 0\n","Non-outlier observations: 500\n","Identified outliers: 0\n","Outliers: \n"," []\n","Y_outliers: \n"," [nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan nan\n"," nan nan nan nan nan nan nan nan nan nan nan nan nan nan]\n","len pos_outliers: \n"," 0\n","len pos_zeros: \n"," 0\n","df_summary: \n","               filename # outliers  ... lower_cutoff  upper_cutoff\n","0   ../data/D1-150.csv         40  ...       -15.00        209.00\n","1   ../data/D1-250.csv         60  ...       -52.00        244.00\n","2   ../data/D1-400.csv        100  ...      -114.50        297.50\n","3    ../data/D1-50.csv         28  ...        34.62        163.62\n","4   ../data/D2-150.csv         33  ...        76.00        124.00\n","5   ../data/D2-250.csv        114  ...        80.00        120.00\n","6   ../data/D2-400.csv        117  ...        77.50        121.50\n","7    ../data/D2-50.csv          4  ...        72.88        125.88\n","8   ../data/D3-150.csv         40  ...        77.50        121.50\n","9   ../data/D3-250.csv         97  ...        80.00        120.00\n","10  ../data/D3-400.csv        128  ...        77.50        121.50\n","11   ../data/D3-50.csv         16  ...        76.00        124.00\n","12  ../data/D4-150.csv          0  ...        53.00        149.00\n","13  ../data/D4-250.csv         50  ...        70.50        130.50\n","14  ../data/D4-400.csv          0  ...        43.50        153.50\n","15   ../data/D4-50.csv          0  ...        22.50        178.50\n","\n","[16 rows x 8 columns]\n","filename:  ../data/D1-150.csv\n","special data:  [[  79    7   10  341]\n"," [ 129   12   10  236]\n"," [ 259   25   10  324]\n"," [ 317   31    8  214]\n"," [ 318   31    9  249]\n"," [ 438   43    9  213]\n"," [ 498   49    9  212]\n"," [ 508   50    9  216]\n"," [ 549   54   10  283]\n"," [ 739   73   10  289]\n"," [ 749   74   10  327]\n"," [ 759   75   10  387]\n"," [ 778   77    9  231]\n"," [ 808   80    9  209]\n"," [ 809   80   10  338]\n"," [ 829   82   10  605]\n"," [ 899   89   10  340]\n"," [ 919   91   10  588]\n"," [ 929   92   10  309]\n"," [ 978   97    9  242]\n"," [1008  100    9  227]\n"," [1039  103   10  210]\n"," [1059  105   10  238]\n"," [1099  109   10  214]\n"," [1108  110    9  236]\n"," [1118  111    9  234]\n"," [1119  111   10  284]\n"," [1149  114   10  291]\n"," [1239  123   10  343]\n"," [1249  124   10  508]\n"," [1258  125    9  251]\n"," [1278  127    9  237]\n"," [1289  128   10  928]\n"," [1309  130   10  472]\n"," [1329  132   10  308]\n"," [1379  137   10  542]\n"," [1409  140   10  288]\n"," [1469  146   10  424]\n"," [1489  148   10  242]\n"," [1499  149   10  221]]\n","df:\n","      pos  row  column  value\n","0     79    7      10    341\n","1    129   12      10    236\n","2    259   25      10    324\n","3    317   31       8    214\n","4    318   31       9    249\n","5    438   43       9    213\n","6    498   49       9    212\n","7    508   50       9    216\n","8    549   54      10    283\n","9    739   73      10    289\n","10   749   74      10    327\n","11   759   75      10    387\n","12   778   77       9    231\n","13   808   80       9    209\n","14   809   80      10    338\n","15   829   82      10    605\n","16   899   89      10    340\n","17   919   91      10    588\n","18   929   92      10    309\n","19   978   97       9    242\n","20  1008  100       9    227\n","21  1039  103      10    210\n","22  1059  105      10    238\n","23  1099  109      10    214\n","24  1108  110       9    236\n","25  1118  111       9    234\n","26  1119  111      10    284\n","27  1149  114      10    291\n","28  1239  123      10    343\n","29  1249  124      10    508\n","30  1258  125       9    251\n","31  1278  127       9    237\n","32  1289  128      10    928\n","33  1309  130      10    472\n","34  1329  132      10    308\n","35  1379  137      10    542\n","36  1409  140      10    288\n","37  1469  146      10    424\n","38  1489  148      10    242\n","39  1499  149      10    221\n","special data:  [[  19    1   10    0]\n"," [  49    4   10    0]\n"," [  68    6    9    0]\n"," [  69    6   10    0]\n"," [  89    8   10    0]\n"," [ 119   11   10    0]\n"," [ 139   13   10    0]\n"," [ 169   16   10    0]\n"," [ 178   17    9    0]\n"," [ 179   17   10    0]\n"," [ 189   18   10    0]\n"," [ 199   19   10    0]\n"," [ 209   20   10    0]\n"," [ 218   21    9    0]\n"," [ 219   21   10    0]\n"," [ 229   22   10    0]\n"," [ 239   23   10    0]\n"," [ 249   24   10    0]\n"," [ 269   26   10    0]\n"," [ 277   27    8    0]\n"," [ 278   27    9    0]\n"," [ 279   27   10    0]\n"," [ 289   28   10    0]\n"," [ 299   29   10    0]\n"," [ 309   30   10    0]\n"," [ 339   33   10    0]\n"," [ 349   34   10    0]\n"," [ 389   38   10    0]\n"," [ 409   40   10    0]\n"," [ 429   42   10    0]\n"," [ 447   44    8    0]\n"," [ 448   44    9    0]\n"," [ 449   44   10    0]\n"," [ 469   46   10    0]\n"," [ 479   47   10    0]\n"," [ 569   56   10    0]\n"," [ 579   57   10    0]\n"," [ 599   59   10    0]\n"," [ 639   63   10    0]\n"," [ 649   64   10    0]\n"," [ 659   65   10    0]\n"," [ 688   68    9    0]\n"," [ 689   68   10    0]\n"," [ 709   70   10    0]\n"," [ 718   71    9    0]\n"," [ 719   71   10    0]\n"," [ 769   76   10    0]\n"," [ 819   81   10    0]\n"," [ 839   83   10    0]\n"," [ 849   84   10    0]\n"," [ 888   88    9    0]\n"," [ 889   88   10    0]\n"," [ 909   90   10    0]\n"," [ 939   93   10    0]\n"," [ 949   94   10    0]\n"," [ 969   96   10    0]\n"," [1019  101   10    0]\n"," [1029  102   10    0]\n"," [1079  107   10    0]\n"," [1089  108   10    0]\n"," [1138  113    9    0]\n"," [1139  113   10    0]\n"," [1179  117   10    0]\n"," [1199  119   10    0]\n"," [1209  120   10    0]\n"," [1269  126   10    0]\n"," [1283  128    4    0]\n"," [1284  128    5    0]\n"," [1285  128    6    0]\n"," [1286  128    7    0]\n"," [1287  128    8    0]\n"," [1368  136    9    0]\n"," [1428  142    9    0]]\n","df:\n","      pos  row  column  value\n","0     19    1      10      0\n","1     49    4      10      0\n","2     68    6       9      0\n","3     69    6      10      0\n","4     89    8      10      0\n","..   ...  ...     ...    ...\n","68  1285  128       6      0\n","69  1286  128       7      0\n","70  1287  128       8      0\n","71  1368  136       9      0\n","72  1428  142       9      0\n","\n","[73 rows x 4 columns]\n","filename:  ../data/D1-250.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[  79    7   10  297]\n"," [ 199   19   10  352]\n"," [ 629   62   10  487]\n"," [ 728   72    9  269]\n"," [ 739   73   10  327]\n"," [ 869   86   10  551]\n"," [ 889   88   10  561]\n"," [ 969   96   10  245]\n"," [1019  101   10  430]\n"," [1028  102    9  355]\n"," [1039  103   10  249]\n"," [1239  123   10  262]\n"," [1248  124    9  273]\n"," [1286  128    7  246]\n"," [1389  138   10  245]\n"," [1449  144   10  630]\n"," [1489  148   10  253]\n"," [1549  154   10  260]\n"," [1609  160   10  330]\n"," [1718  171    9  267]\n"," [1767  176    8  266]\n"," [1789  178   10  707]\n"," [1809  180   10  341]\n"," [1828  182    9  254]\n"," [1839  183   10  413]\n"," [1849  184   10  304]\n"," [1857  185    8  292]\n"," [1889  188   10  273]\n"," [1890  189    1  260]\n"," [1978  197    9  293]\n"," [1989  198   10  824]\n"," [2019  201   10  632]\n"," [2029  202   10  336]\n"," [2048  204    9  346]\n"," [2058  205    9  266]\n"," [2068  206    9  325]\n"," [2069  206   10  275]\n"," [2078  207    9  344]\n"," [2119  211   10  845]\n"," [2129  212   10  293]\n"," [2149  214   10  765]\n"," [2159  215   10  308]\n"," [2168  216    9  272]\n"," [2169  216   10  334]\n"," [2179  217   10  286]\n"," [2189  218   10  323]\n"," [2219  221   10  318]\n"," [2269  226   10  370]\n"," [2309  230   10  449]\n"," [2319  231   10  251]\n"," [2329  232   10  658]\n"," [2349  234   10  326]\n"," [2359  235   10  438]\n"," [2369  236   10  289]\n"," [2399  239   10  244]\n"," [2419  241   10  409]\n"," [2439  243   10  256]\n"," [2449  244   10  303]\n"," [2459  245   10  405]\n"," [2489  248   10  424]]\n","df:\n","      pos  row  column  value\n","0     79    7      10    297\n","1    199   19      10    352\n","2    629   62      10    487\n","3    728   72       9    269\n","4    739   73      10    327\n","5    869   86      10    551\n","6    889   88      10    561\n","7    969   96      10    245\n","8   1019  101      10    430\n","9   1028  102       9    355\n","10  1039  103      10    249\n","11  1239  123      10    262\n","12  1248  124       9    273\n","13  1286  128       7    246\n","14  1389  138      10    245\n","15  1449  144      10    630\n","16  1489  148      10    253\n","17  1549  154      10    260\n","18  1609  160      10    330\n","19  1718  171       9    267\n","20  1767  176       8    266\n","21  1789  178      10    707\n","22  1809  180      10    341\n","23  1828  182       9    254\n","24  1839  183      10    413\n","25  1849  184      10    304\n","26  1857  185       8    292\n","27  1889  188      10    273\n","28  1890  189       1    260\n","29  1978  197       9    293\n","30  1989  198      10    824\n","31  2019  201      10    632\n","32  2029  202      10    336\n","33  2048  204       9    346\n","34  2058  205       9    266\n","35  2068  206       9    325\n","36  2069  206      10    275\n","37  2078  207       9    344\n","38  2119  211      10    845\n","39  2129  212      10    293\n","40  2149  214      10    765\n","41  2159  215      10    308\n","42  2168  216       9    272\n","43  2169  216      10    334\n","44  2179  217      10    286\n","45  2189  218      10    323\n","46  2219  221      10    318\n","47  2269  226      10    370\n","48  2309  230      10    449\n","49  2319  231      10    251\n","50  2329  232      10    658\n","51  2349  234      10    326\n","52  2359  235      10    438\n","53  2369  236      10    289\n","54  2399  239      10    244\n","55  2419  241      10    409\n","56  2439  243      10    256\n","57  2449  244      10    303\n","58  2459  245      10    405\n","59  2489  248      10    424\n","special data:  [[   6    0    7    0]\n"," [   7    0    8    0]\n"," [   8    0    9    0]\n"," ...\n"," [2468  246    9    0]\n"," [2488  248    9    0]\n"," [2498  249    9    0]]\n","df:\n","       pos  row  column  value\n","0       6    0       7      0\n","1       7    0       8      0\n","2       8    0       9      0\n","3       9    0      10      0\n","4      28    2       9      0\n","..    ...  ...     ...    ...\n","264  2378  237       9      0\n","265  2398  239       9      0\n","266  2468  246       9      0\n","267  2488  248       9      0\n","268  2498  249       9      0\n","\n","[269 rows x 4 columns]\n","filename:  ../data/D1-400.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[ 208   20    9  305]\n"," [ 298   29    9  312]\n"," [ 378   37    9  314]\n"," [ 388   38    9  315]\n"," [ 712   71    3  308]\n"," [ 848   84    9  331]\n"," [ 948   94    9  340]\n"," [ 968   96    9  341]\n"," [1048  104    9  341]\n"," [1088  108    9  331]\n"," [1098  109    9  328]\n"," [1108  110    9  322]\n"," [1208  120    9  335]\n"," [1278  127    9  336]\n"," [1328  132    9  327]\n"," [1348  134    9  325]\n"," [1358  135    9  312]\n"," [1468  146    9  309]\n"," [1598  159    9  308]\n"," [1638  163    9  311]\n"," [1688  168    9  310]\n"," [1729  172   10  302]\n"," [1769  176   10  303]\n"," [1959  195   10  335]\n"," [1999  199   10  392]\n"," [2219  221   10  395]\n"," [2319  231   10  425]\n"," [2389  238   10  470]\n"," [2467  246    8  330]\n"," [2499  249   10  489]\n"," [2529  252   10  492]\n"," [2539  253   10  322]\n"," [2549  254   10  383]\n"," [2699  269   10  520]\n"," [2719  271   10  430]\n"," [2747  274    8  336]\n"," [2867  286    8  302]\n"," [2928  292    9  315]\n"," [2948  294    9  305]\n"," [2958  295    9  326]\n"," [2988  298    9  316]\n"," [2989  298   10  359]\n"," [3018  301    9  324]\n"," [3019  301   10  476]\n"," [3028  302    9  324]\n"," [3029  302   10  309]\n"," [3058  305    9  328]\n"," [3059  305   10  615]\n"," [3068  306    9  328]\n"," [3078  307    9  318]\n"," [3088  308    9  331]\n"," [3108  310    9  327]\n"," [3109  310   10  627]\n"," [3188  318    9  367]\n"," [3189  318   10  377]\n"," [3198  319    9  368]\n"," [3208  320    9  346]\n"," [3218  321    9  349]\n"," [3248  324    9  338]\n"," [3249  324   10  662]\n"," [3258  325    9  319]\n"," [3268  326    9  312]\n"," [3288  328    9  308]\n"," [3309  330   10  706]\n"," [3318  331    9  321]\n"," [3329  332   10  698]\n"," [3389  338   10  652]\n"," [3409  340   10  811]\n"," [3429  342   10  398]\n"," [3449  344   10  746]\n"," [3469  346   10  744]\n"," [3479  347   10  900]\n"," [3509  350   10  647]\n"," [3529  352   10  599]\n"," [3539  353   10  308]\n"," [3549  354   10  315]\n"," [3559  355   10  909]\n"," [3569  356   10  931]\n"," [3589  358   10  477]\n"," [3599  359   10  306]\n"," [3609  360   10  396]\n"," [3639  363   10  508]\n"," [3649  364   10  437]\n"," [3689  368   10  338]\n"," [3699  369   10  493]\n"," [3739  373   10 1000]\n"," [3749  374   10  401]\n"," [3769  376   10  319]\n"," [3789  378   10 1000]\n"," [3799  379   10 1000]\n"," [3809  380   10  607]\n"," [3849  384   10  452]\n"," [3869  386   10  382]\n"," [3879  387   10 1000]\n"," [3889  388   10  370]\n"," [3899  389   10  759]\n"," [3919  391   10  435]\n"," [3949  394   10  955]\n"," [3959  395   10  962]\n"," [3969  396   10  891]]\n","df:\n","      pos  row  column  value\n","0    208   20       9    305\n","1    298   29       9    312\n","2    378   37       9    314\n","3    388   38       9    315\n","4    712   71       3    308\n","..   ...  ...     ...    ...\n","95  3899  389      10    759\n","96  3919  391      10    435\n","97  3949  394      10    955\n","98  3959  395      10    962\n","99  3969  396      10    891\n","\n","[100 rows x 4 columns]\n","special data:  [[   6    0    7    0]\n"," [   7    0    8    0]\n"," [   8    0    9    0]\n"," ...\n"," [3978  397    9    0]\n"," [3997  399    8    0]\n"," [3998  399    9    0]]\n","df:\n","       pos  row  column  value\n","0       6    0       7      0\n","1       7    0       8      0\n","2       8    0       9      0\n","3       9    0      10      0\n","4      18    1       9      0\n","..    ...  ...     ...    ...\n","821  3976  397       7      0\n","822  3977  397       8      0\n","823  3978  397       9      0\n","824  3997  399       8      0\n","825  3998  399       9      0\n","\n","[826 rows x 4 columns]\n","filename:  ../data/D1-50.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[ 18   1   9 175]\n"," [ 39   3  10  16]\n"," [ 49   4  10  22]\n"," [ 79   7  10   0]\n"," [ 89   8  10 291]\n"," [109  10  10 191]\n"," [119  11  10 293]\n"," [149  14  10   0]\n"," [169  16  10  32]\n"," [189  18  10 198]\n"," [199  19  10  28]\n"," [208  20   9 168]\n"," [218  21   9  26]\n"," [219  21  10   0]\n"," [269  26  10 230]\n"," [279  27  10 176]\n"," [308  30   9 181]\n"," [318  31   9 178]\n"," [329  32  10  27]\n"," [349  34  10  21]\n"," [379  37  10 192]\n"," [389  38  10   0]\n"," [409  40  10  31]\n"," [447  44   8  27]\n"," [448  44   9  26]\n"," [459  45  10 276]\n"," [469  46  10 172]\n"," [479  47  10 179]]\n","df:\n","     pos  row  column  value\n","0    18    1       9    175\n","1    39    3      10     16\n","2    49    4      10     22\n","3    79    7      10      0\n","4    89    8      10    291\n","5   109   10      10    191\n","6   119   11      10    293\n","7   149   14      10      0\n","8   169   16      10     32\n","9   189   18      10    198\n","10  199   19      10     28\n","11  208   20       9    168\n","12  218   21       9     26\n","13  219   21      10      0\n","14  269   26      10    230\n","15  279   27      10    176\n","16  308   30       9    181\n","17  318   31       9    178\n","18  329   32      10     27\n","19  349   34      10     21\n","20  379   37      10    192\n","21  389   38      10      0\n","22  409   40      10     31\n","23  447   44       8     27\n","24  448   44       9     26\n","25  459   45      10    276\n","26  469   46      10    172\n","27  479   47      10    179\n","special data:  [[ 79   7  10   0]\n"," [149  14  10   0]\n"," [219  21  10   0]\n"," [389  38  10   0]]\n","df:\n","    pos  row  column  value\n","0   79    7      10      0\n","1  149   14      10      0\n","2  219   21      10      0\n","3  389   38      10      0\n","filename:  ../data/D2-150.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[   0    0    1   54]\n"," [  30    3    1  142]\n"," [ 192   19    3   76]\n"," [ 210   21    1   72]\n"," [ 212   21    3   68]\n"," [ 250   25    1   73]\n"," [ 315   31    6   76]\n"," [ 317   31    8   76]\n"," [ 319   31   10   76]\n"," [ 372   37    3   72]\n"," [ 512   51    3   71]\n"," [ 632   63    3  126]\n"," [ 670   67    1  128]\n"," [ 675   67    6   73]\n"," [ 677   67    8   73]\n"," [ 679   67   10   73]\n"," [ 840   84    1   74]\n"," [ 882   88    3  127]\n"," [ 933   93    4  130]\n"," [ 983   98    4  128]\n"," [1010  101    1   72]\n"," [1070  107    1   75]\n"," [1172  117    3  148]\n"," [1202  120    3  130]\n"," [1225  122    6   70]\n"," [1227  122    8   70]\n"," [1229  122   10   70]\n"," [1373  137    4  127]\n"," [1390  139    1  124]\n"," [1412  141    3  124]\n"," [1482  148    3   70]\n"," [1483  148    4  131]\n"," [1492  149    3  133]]\n","df:\n","      pos  row  column  value\n","0      0    0       1     54\n","1     30    3       1    142\n","2    192   19       3     76\n","3    210   21       1     72\n","4    212   21       3     68\n","5    250   25       1     73\n","6    315   31       6     76\n","7    317   31       8     76\n","8    319   31      10     76\n","9    372   37       3     72\n","10   512   51       3     71\n","11   632   63       3    126\n","12   670   67       1    128\n","13   675   67       6     73\n","14   677   67       8     73\n","15   679   67      10     73\n","16   840   84       1     74\n","17   882   88       3    127\n","18   933   93       4    130\n","19   983   98       4    128\n","20  1010  101       1     72\n","21  1070  107       1     75\n","22  1172  117       3    148\n","23  1202  120       3    130\n","24  1225  122       6     70\n","25  1227  122       8     70\n","26  1229  122      10     70\n","27  1373  137       4    127\n","28  1390  139       1    124\n","29  1412  141       3    124\n","30  1482  148       3     70\n","31  1483  148       4    131\n","32  1492  149       3    133\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D2-250.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[  35    3    6  125]\n"," [  37    3    8  125]\n"," [  39    3   10  125]\n"," [  42    4    3  122]\n"," [ 140   14    1  122]\n"," [ 145   14    6   80]\n"," [ 147   14    8   80]\n"," [ 149   14   10   80]\n"," [ 170   17    1  131]\n"," [ 200   20    1  124]\n"," [ 220   22    1  123]\n"," [ 250   25    1   77]\n"," [ 253   25    4  127]\n"," [ 272   27    3  128]\n"," [ 275   27    6   80]\n"," [ 313   31    4   65]\n"," [ 352   35    3  122]\n"," [ 440   44    1   65]\n"," [ 443   44    4  136]\n"," [ 445   44    6  122]\n"," [ 447   44    8  122]\n"," [ 449   44   10  122]\n"," [ 462   46    3   79]\n"," [ 490   49    1  122]\n"," [ 500   50    1  122]\n"," [ 510   51    1   72]\n"," [ 570   57    1   79]\n"," [ 572   57    3   79]\n"," [ 573   57    4  121]\n"," [ 580   58    1  121]\n"," [ 600   60    1   74]\n"," [ 610   61    1   72]\n"," [ 670   67    1  127]\n"," [ 690   69    1  121]\n"," [ 720   72    1   77]\n"," [ 745   74    6  122]\n"," [ 747   74    8  122]\n"," [ 749   74   10  122]\n"," [ 813   81    4  121]\n"," [ 842   84    3  121]\n"," [ 862   86    3   78]\n"," [ 905   90    6  120]\n"," [ 907   90    8  120]\n"," [ 909   90   10  120]\n"," [ 912   91    3  123]\n"," [ 983   98    4  120]\n"," [1045  104    6  120]\n"," [1047  104    8  120]\n"," [1049  104   10  121]\n"," [1052  105    3  128]\n"," [1065  106    6  120]\n"," [1067  106    8  120]\n"," [1069  106   10  120]\n"," [1210  121    1  120]\n"," [1212  121    3  125]\n"," [1215  121    6   76]\n"," [1217  121    8   76]\n"," [1219  121   10   76]\n"," [1293  129    4   76]\n"," [1312  131    3  125]\n"," [1320  132    1   71]\n"," [1322  132    3   80]\n"," [1362  136    3   68]\n"," [1390  139    1  120]\n"," [1455  145    6   80]\n"," [1457  145    8   80]\n"," [1459  145   10   80]\n"," [1470  147    1  127]\n"," [1482  148    3   62]\n"," [1530  153    1  120]\n"," [1600  160    1   70]\n"," [1602  160    3   77]\n"," [1652  165    3   71]\n"," [1655  165    6  120]\n"," [1657  165    8  120]\n"," [1659  165   10  120]\n"," [1683  168    4   79]\n"," [1720  172    1   77]\n"," [1750  175    1   78]\n"," [1800  180    1   76]\n"," [1810  181    1  127]\n"," [1815  181    6   80]\n"," [1817  181    8   79]\n"," [1819  181   10   79]\n"," [1820  182    1   76]\n"," [1832  183    3  122]\n"," [1862  186    3   80]\n"," [1972  197    3  136]\n"," [1975  197    6   71]\n"," [1977  197    8   70]\n"," [1979  197   10   70]\n"," [2053  205    4  122]\n"," [2065  206    6   80]\n"," [2067  206    8   80]\n"," [2069  206   10   80]\n"," [2132  213    3   60]\n"," [2155  215    6  124]\n"," [2157  215    8  124]\n"," [2159  215   10  124]\n"," [2160  216    1  125]\n"," [2222  222    3  132]\n"," [2293  229    4  126]\n"," [2300  230    1   69]\n"," [2302  230    3   75]\n"," [2350  235    1  120]\n"," [2352  235    3  120]\n"," [2363  236    4  121]\n"," [2410  241    1   72]\n"," [2440  244    1  123]\n"," [2462  246    3  132]\n"," [2492  249    3  139]\n"," [2495  249    6   75]\n"," [2497  249    8   76]\n"," [2499  249   10   75]]\n","df:\n","       pos  row  column  value\n","0      35    3       6    125\n","1      37    3       8    125\n","2      39    3      10    125\n","3      42    4       3    122\n","4     140   14       1    122\n","..    ...  ...     ...    ...\n","109  2462  246       3    132\n","110  2492  249       3    139\n","111  2495  249       6     75\n","112  2497  249       8     76\n","113  2499  249      10     75\n","\n","[114 rows x 4 columns]\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D2-400.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[  50    5    1   76]\n"," [ 112   11    3   68]\n"," [ 142   14    3  128]\n"," [ 175   17    6  123]\n"," [ 177   17    8  123]\n"," [ 179   17   10  123]\n"," [ 272   27    3   77]\n"," [ 273   27    4  128]\n"," [ 340   34    1   75]\n"," [ 352   35    3  124]\n"," [ 392   39    3  122]\n"," [ 493   49    4  125]\n"," [ 522   52    3  123]\n"," [ 563   56    4  126]\n"," [ 570   57    1   70]\n"," [ 580   58    1  125]\n"," [ 600   60    1   71]\n"," [ 610   61    1   74]\n"," [ 670   67    1  122]\n"," [ 772   77    3  133]\n"," [ 823   82    4  130]\n"," [ 830   83    1  122]\n"," [ 832   83    3  123]\n"," [ 840   84    1   75]\n"," [1010  101    1  137]\n"," [1032  103    3  124]\n"," [1070  107    1   72]\n"," [1080  108    1  126]\n"," [1082  108    3  141]\n"," [1132  113    3   75]\n"," [1210  121    1   71]\n"," [1212  121    3   72]\n"," [1213  121    4  127]\n"," [1233  123    4   77]\n"," [1260  126    1   75]\n"," [1312  131    3  125]\n"," [1315  131    6   73]\n"," [1317  131    8   73]\n"," [1319  131   10   73]\n"," [1323  132    4   67]\n"," [1343  134    4   74]\n"," [1350  135    1  127]\n"," [1353  135    4   76]\n"," [1442  144    3  122]\n"," [1502  150    3   70]\n"," [1533  153    4   71]\n"," [1803  180    4  124]\n"," [1850  185    1  133]\n"," [1892  189    3  130]\n"," [1900  190    1   77]\n"," [1905  190    6  125]\n"," [1907  190    8  125]\n"," [1909  190   10  125]\n"," [1952  195    3   75]\n"," [1992  199    3   72]\n"," [2010  201    1  127]\n"," [2093  209    4  124]\n"," [2132  213    3   76]\n"," [2140  214    1  122]\n"," [2160  216    1   61]\n"," [2190  219    1   72]\n"," [2203  220    4   75]\n"," [2212  221    3  123]\n"," [2360  236    1  133]\n"," [2362  236    3  130]\n"," [2382  238    3   77]\n"," [2462  246    3  130]\n"," [2482  248    3  134]\n"," [2483  248    4   73]\n"," [2500  250    1  143]\n"," [2612  261    3  125]\n"," [2673  267    4   76]\n"," [2682  268    3  126]\n"," [2692  269    3  124]\n"," [2705  270    6  123]\n"," [2707  270    8  123]\n"," [2709  270   10  123]\n"," [2712  271    3  122]\n"," [2713  271    4  125]\n"," [2722  272    3   71]\n"," [2832  283    3  125]\n"," [2893  289    4  122]\n"," [2922  292    3  133]\n"," [2930  293    1  123]\n"," [2940  294    1   74]\n"," [2963  296    4  122]\n"," [2980  298    1  125]\n"," [2982  298    3  126]\n"," [3013  301    4  129]\n"," [3032  303    3   71]\n"," [3100  310    1  123]\n"," [3150  315    1   77]\n"," [3160  316    1  124]\n"," [3173  317    4   77]\n"," [3192  319    3  135]\n"," [3212  321    3  124]\n"," [3330  333    1  125]\n"," [3352  335    3  126]\n"," [3360  336    1   67]\n"," [3443  344    4  128]\n"," [3492  349    3   75]\n"," [3593  359    4  123]\n"," [3620  362    1   65]\n"," [3633  363    4  122]\n"," [3649  364   10   77]\n"," [3663  366    4  124]\n"," [3712  371    3  123]\n"," [3753  375    4   75]\n"," [3803  380    4   63]\n"," [3840  384    1  125]\n"," [3870  387    1   72]\n"," [3890  389    1   75]\n"," [3920  392    1   75]\n"," [3925  392    6  123]\n"," [3927  392    8  123]\n"," [3929  392   10  122]\n"," [3982  398    3   77]]\n","df:\n","       pos  row  column  value\n","0      50    5       1     76\n","1     112   11       3     68\n","2     142   14       3    128\n","3     175   17       6    123\n","4     177   17       8    123\n","..    ...  ...     ...    ...\n","112  3920  392       1     75\n","113  3925  392       6    123\n","114  3927  392       8    123\n","115  3929  392      10    122\n","116  3982  398       3     77\n","\n","[117 rows x 4 columns]\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D2-50.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[232  23   3  69]\n"," [373  37   4 132]\n"," [440  44   1  69]\n"," [452  45   3 146]]\n","df:\n","    pos  row  column  value\n","0  232   23       3     69\n","1  373   37       4    132\n","2  440   44       1     69\n","3  452   45       3    146\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D3-150.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[   9    0   10  124]\n"," [  53    5    4  137]\n"," [  81    8    2   76]\n"," [  89    8   10  135]\n"," [ 171   17    2  122]\n"," [ 259   25   10   71]\n"," [ 281   28    2   75]\n"," [ 299   29   10  135]\n"," [ 333   33    4  139]\n"," [ 339   33   10   76]\n"," [ 353   35    4  127]\n"," [ 419   41   10   73]\n"," [ 533   53    4  123]\n"," [ 593   59    4  126]\n"," [ 632   63    3   76]\n"," [ 633   63    4   73]\n"," [ 636   63    7   77]\n"," [ 637   63    8  136]\n"," [ 639   63   10  125]\n"," [ 693   69    4  141]\n"," [ 713   71    4   71]\n"," [ 719   71   10  125]\n"," [ 729   72   10  123]\n"," [ 779   77   10  122]\n"," [ 799   79   10   74]\n"," [ 869   86   10  127]\n"," [ 879   87   10  136]\n"," [1083  108    4   75]\n"," [1129  112   10   76]\n"," [1149  114   10   77]\n"," [1153  115    4   72]\n"," [1163  116    4  124]\n"," [1179  117   10   69]\n"," [1211  121    2   73]\n"," [1219  121   10  132]\n"," [1243  124    4   76]\n"," [1273  127    4  128]\n"," [1279  127   10  123]\n"," [1369  136   10   77]\n"," [1457  145    8  122]]\n","df:\n","      pos  row  column  value\n","0      9    0      10    124\n","1     53    5       4    137\n","2     81    8       2     76\n","3     89    8      10    135\n","4    171   17       2    122\n","5    259   25      10     71\n","6    281   28       2     75\n","7    299   29      10    135\n","8    333   33       4    139\n","9    339   33      10     76\n","10   353   35       4    127\n","11   419   41      10     73\n","12   533   53       4    123\n","13   593   59       4    126\n","14   632   63       3     76\n","15   633   63       4     73\n","16   636   63       7     77\n","17   637   63       8    136\n","18   639   63      10    125\n","19   693   69       4    141\n","20   713   71       4     71\n","21   719   71      10    125\n","22   729   72      10    123\n","23   779   77      10    122\n","24   799   79      10     74\n","25   869   86      10    127\n","26   879   87      10    136\n","27  1083  108       4     75\n","28  1129  112      10     76\n","29  1149  114      10     77\n","30  1153  115       4     72\n","31  1163  116       4    124\n","32  1179  117      10     69\n","33  1211  121       2     73\n","34  1219  121      10    132\n","35  1243  124       4     76\n","36  1273  127       4    128\n","37  1279  127      10    123\n","38  1369  136      10     77\n","39  1457  145       8    122\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D3-250.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[   3    0    4  124]\n"," [   9    0   10   69]\n"," [  19    1   10   80]\n"," [  59    5   10  128]\n"," [  89    8   10  123]\n"," [  97    9    8  120]\n"," [  99    9   10  131]\n"," [ 109   10   10   79]\n"," [ 123   12    4   73]\n"," [ 133   13    4   66]\n"," [ 153   15    4   76]\n"," [ 163   16    4   76]\n"," [ 169   16   10   80]\n"," [ 173   17    4   62]\n"," [ 185   18    6  121]\n"," [ 193   19    4  122]\n"," [ 203   20    4  120]\n"," [ 303   30    4  123]\n"," [ 307   30    8   79]\n"," [ 353   35    4  128]\n"," [ 358   35    9   80]\n"," [ 419   41   10  123]\n"," [ 453   45    4  134]\n"," [ 489   48   10  126]\n"," [ 503   50    4  124]\n"," [ 509   50   10   75]\n"," [ 533   53    4   80]\n"," [ 545   54    6   80]\n"," [ 549   54   10   77]\n"," [ 553   55    4  123]\n"," [ 599   59   10   79]\n"," [ 697   69    8  122]\n"," [ 713   71    4  128]\n"," [ 773   77    4   80]\n"," [ 783   78    4  124]\n"," [ 809   80   10  120]\n"," [ 819   81   10  132]\n"," [ 859   85   10  126]\n"," [ 869   86   10   78]\n"," [ 979   97   10  128]\n"," [ 993   99    4   80]\n"," [1009  100   10   72]\n"," [1013  101    4   74]\n"," [1053  105    4   80]\n"," [1104  110    5  121]\n"," [1105  110    6   76]\n"," [1113  111    4   65]\n"," [1133  113    4   76]\n"," [1173  117    4  125]\n"," [1179  117   10   78]\n"," [1299  129   10  124]\n"," [1319  131   10   75]\n"," [1343  134    4  120]\n"," [1443  144    4  124]\n"," [1452  145    3   79]\n"," [1456  145    7   80]\n"," [1459  145   10  128]\n"," [1479  147   10   72]\n"," [1493  149    4  121]\n"," [1519  151   10  122]\n"," [1649  164   10  124]\n"," [1669  166   10   78]\n"," [1739  173   10  128]\n"," [1781  178    2  123]\n"," [1789  178   10   68]\n"," [1831  183    2  123]\n"," [1859  185   10   76]\n"," [1869  186   10  123]\n"," [1899  189   10  122]\n"," [1933  193    4  130]\n"," [1959  195   10   79]\n"," [2013  201    4   80]\n"," [2043  204    4   78]\n"," [2049  204   10  122]\n"," [2069  206   10  121]\n"," [2079  207   10  120]\n"," [2091  209    2  129]\n"," [2099  209   10   65]\n"," [2139  213   10  122]\n"," [2153  215    4   73]\n"," [2159  215   10  137]\n"," [2203  220    4  127]\n"," [2213  221    4   76]\n"," [2223  222    4  120]\n"," [2239  223   10  120]\n"," [2243  224    4  124]\n"," [2249  224   10   76]\n"," [2251  225    2   79]\n"," [2279  227   10  121]\n"," [2289  228   10   58]\n"," [2333  233    4  127]\n"," [2337  233    8   73]\n"," [2339  233   10  138]\n"," [2389  238   10  122]\n"," [2393  239    4   80]\n"," [2443  244    4   77]\n"," [2479  247   10   75]]\n","df:\n","      pos  row  column  value\n","0      3    0       4    124\n","1      9    0      10     69\n","2     19    1      10     80\n","3     59    5      10    128\n","4     89    8      10    123\n","..   ...  ...     ...    ...\n","92  2339  233      10    138\n","93  2389  238      10    122\n","94  2393  239       4     80\n","95  2443  244       4     77\n","96  2479  247      10     75\n","\n","[97 rows x 4 columns]\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D3-400.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[   9    0   10  122]\n"," [ 133   13    4  155]\n"," [ 153   15    4   76]\n"," [ 173   17    4  127]\n"," [ 203   20    4   75]\n"," [ 231   23    2   77]\n"," [ 251   25    2  124]\n"," [ 253   25    4   72]\n"," [ 257   25    8  132]\n"," [ 259   25   10   75]\n"," [ 269   26   10   74]\n"," [ 309   30   10   67]\n"," [ 391   39    2  127]\n"," [ 429   42   10  123]\n"," [ 493   49    4  125]\n"," [ 549   54   10  123]\n"," [ 573   57    4  124]\n"," [ 599   59   10   70]\n"," [ 603   60    4  141]\n"," [ 609   60   10   75]\n"," [ 619   61   10  133]\n"," [ 633   63    4  138]\n"," [ 689   68   10   76]\n"," [ 709   70   10   77]\n"," [ 803   80    4  130]\n"," [ 853   85    4   76]\n"," [ 857   85    8  122]\n"," [ 883   88    4  125]\n"," [ 903   90    4  127]\n"," [ 915   91    6  122]\n"," [ 987   98    8  122]\n"," [1013  101    4  123]\n"," [1039  103   10  129]\n"," [1137  113    8   74]\n"," [1181  118    2  122]\n"," [1183  118    4   67]\n"," [1201  120    2  123]\n"," [1209  120   10   77]\n"," [1223  122    4   72]\n"," [1229  122   10  128]\n"," [1261  126    2   74]\n"," [1273  127    4   77]\n"," [1277  127    8  131]\n"," [1281  128    2  132]\n"," [1299  129   10  122]\n"," [1381  138    2  123]\n"," [1423  142    4   74]\n"," [1449  144   10   73]\n"," [1463  146    4   72]\n"," [1564  156    5   70]\n"," [1565  156    6  123]\n"," [1619  161   10  122]\n"," [1743  174    4   74]\n"," [1793  179    4  128]\n"," [1803  180    4   70]\n"," [1807  180    8  123]\n"," [1833  183    4  128]\n"," [1849  184   10  122]\n"," [2009  200   10  128]\n"," [2017  201    8   77]\n"," [2023  202    4  123]\n"," [2043  204    4  125]\n"," [2123  212    4  124]\n"," [2139  213   10   76]\n"," [2153  215    4  143]\n"," [2189  218   10  123]\n"," [2193  219    4  130]\n"," [2203  220    4   74]\n"," [2211  221    2  128]\n"," [2213  221    4   77]\n"," [2217  221    8  123]\n"," [2259  225   10   73]\n"," [2329  232   10  123]\n"," [2363  236    4  126]\n"," [2369  236   10  127]\n"," [2373  237    4  130]\n"," [2403  240    4   77]\n"," [2409  240   10  139]\n"," [2421  242    2  126]\n"," [2423  242    4   75]\n"," [2433  243    4  131]\n"," [2513  251    4   74]\n"," [2589  258   10  124]\n"," [2593  259    4   73]\n"," [2603  260    4  128]\n"," [2699  269   10  124]\n"," [2719  271   10   75]\n"," [2759  275   10   75]\n"," [2813  281    4   72]\n"," [2873  287    4   76]\n"," [2903  290    4  122]\n"," [3003  300    4  122]\n"," [3017  301    8  122]\n"," [3033  303    4   77]\n"," [3035  303    6  123]\n"," [3059  305   10  126]\n"," [3103  310    4   68]\n"," [3107  310    8  127]\n"," [3169  316   10   76]\n"," [3239  323   10  122]\n"," [3309  330   10   77]\n"," [3323  332    4   76]\n"," [3339  333   10  123]\n"," [3389  338   10  129]\n"," [3403  340    4   75]\n"," [3451  345    2  131]\n"," [3459  345   10   76]\n"," [3463  346    4  133]\n"," [3469  346   10   74]\n"," [3479  347   10  125]\n"," [3509  350   10  127]\n"," [3523  352    4  124]\n"," [3603  360    4  126]\n"," [3627  362    8   77]\n"," [3653  365    4   72]\n"," [3713  371    4  137]\n"," [3719  371   10   73]\n"," [3763  376    4   75]\n"," [3829  382   10  131]\n"," [3860  386    1  122]\n"," [3861  386    2   71]\n"," [3863  386    4   76]\n"," [3868  386    9  122]\n"," [3869  386   10  132]\n"," [3939  393   10   75]\n"," [3943  394    4  122]\n"," [3947  394    8   75]\n"," [3999  399   10   67]]\n","df:\n","       pos  row  column  value\n","0       9    0      10    122\n","1     133   13       4    155\n","2     153   15       4     76\n","3     173   17       4    127\n","4     203   20       4     75\n","..    ...  ...     ...    ...\n","123  3869  386      10    132\n","124  3939  393      10     75\n","125  3943  394       4    122\n","126  3947  394       8     75\n","127  3999  399      10     67\n","\n","[128 rows x 4 columns]\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D3-50.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[ 19   1  10  69]\n"," [ 91   9   2  73]\n"," [139  13  10  74]\n"," [143  14   4  69]\n"," [147  14   8 127]\n"," [153  15   4  76]\n"," [199  19  10 132]\n"," [252  25   3 124]\n"," [253  25   4 128]\n"," [256  25   7 129]\n"," [257  25   8  65]\n"," [259  25  10  69]\n"," [283  28   4  72]\n"," [319  31  10 130]\n"," [361  36   2  70]\n"," [497  49   8 129]]\n","df:\n","     pos  row  column  value\n","0    19    1      10     69\n","1    91    9       2     73\n","2   139   13      10     74\n","3   143   14       4     69\n","4   147   14       8    127\n","5   153   15       4     76\n","6   199   19      10    132\n","7   252   25       3    124\n","8   253   25       4    128\n","9   256   25       7    129\n","10  257   25       8     65\n","11  259   25      10     69\n","12  283   28       4     72\n","13  319   31      10    130\n","14  361   36       2     70\n","15  497   49       8    129\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D4-150.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D4-250.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  [[ 174   17    5   63]\n"," [ 184   18    5   63]\n"," [ 194   19    5   58]\n"," [ 204   20    5   68]\n"," [ 214   21    5   66]\n"," [ 425   42    6   63]\n"," [ 435   43    6   63]\n"," [ 445   44    6   58]\n"," [ 455   45    6   68]\n"," [ 465   46    6   66]\n"," [ 676   67    7   63]\n"," [ 686   68    7   63]\n"," [ 696   69    7   58]\n"," [ 706   70    7   68]\n"," [ 716   71    7   66]\n"," [ 927   92    8   63]\n"," [ 937   93    8   63]\n"," [ 947   94    8   58]\n"," [ 957   95    8   68]\n"," [ 967   96    8   66]\n"," [1178  117    9   63]\n"," [1188  118    9   63]\n"," [1198  119    9   58]\n"," [1208  120    9   68]\n"," [1218  121    9   66]\n"," [1429  142   10   63]\n"," [1439  143   10   63]\n"," [1449  144   10   58]\n"," [1459  145   10   68]\n"," [1469  146   10   66]\n"," [1670  167    1   63]\n"," [1680  168    1   63]\n"," [1690  169    1   58]\n"," [1700  170    1   68]\n"," [1710  171    1   66]\n"," [1921  192    2   63]\n"," [1931  193    2   63]\n"," [1941  194    2   58]\n"," [1951  195    2   68]\n"," [1961  196    2   66]\n"," [2172  217    3   63]\n"," [2182  218    3   63]\n"," [2192  219    3   58]\n"," [2202  220    3   68]\n"," [2212  221    3   66]\n"," [2423  242    4   63]\n"," [2433  243    4   63]\n"," [2443  244    4   58]\n"," [2453  245    4   68]\n"," [2463  246    4   66]]\n","df:\n","      pos  row  column  value\n","0    174   17       5     63\n","1    184   18       5     63\n","2    194   19       5     58\n","3    204   20       5     68\n","4    214   21       5     66\n","5    425   42       6     63\n","6    435   43       6     63\n","7    445   44       6     58\n","8    455   45       6     68\n","9    465   46       6     66\n","10   676   67       7     63\n","11   686   68       7     63\n","12   696   69       7     58\n","13   706   70       7     68\n","14   716   71       7     66\n","15   927   92       8     63\n","16   937   93       8     63\n","17   947   94       8     58\n","18   957   95       8     68\n","19   967   96       8     66\n","20  1178  117       9     63\n","21  1188  118       9     63\n","22  1198  119       9     58\n","23  1208  120       9     68\n","24  1218  121       9     66\n","25  1429  142      10     63\n","26  1439  143      10     63\n","27  1449  144      10     58\n","28  1459  145      10     68\n","29  1469  146      10     66\n","30  1670  167       1     63\n","31  1680  168       1     63\n","32  1690  169       1     58\n","33  1700  170       1     68\n","34  1710  171       1     66\n","35  1921  192       2     63\n","36  1931  193       2     63\n","37  1941  194       2     58\n","38  1951  195       2     68\n","39  1961  196       2     66\n","40  2172  217       3     63\n","41  2182  218       3     63\n","42  2192  219       3     58\n","43  2202  220       3     68\n","44  2212  221       3     66\n","45  2423  242       4     63\n","46  2433  243       4     63\n","47  2443  244       4     58\n","48  2453  245       4     68\n","49  2463  246       4     66\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D4-400.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","filename:  ../data/D4-50.csv\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:354: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 1)\n","/content/drive/MyDrive/colab_wolfs_git_clones/lizard/statistical_analysis/linear_regression/linear_regression_analyze_tp.py:358: MatplotlibDeprecationWarning: Adding an axes using the same arguments as a previous axes currently reuses the earlier instance.  In a future version, a new instance will always be created and returned.  Meanwhile, this warning can be suppressed, and the future behavior ensured, by passing a unique label to each axes instance.\n","  plt.subplot(1, 2, 2)\n"]},{"output_type":"stream","name":"stdout","text":["special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n","special data:  []\n","df:\n"," Empty DataFrame\n","Columns: [pos, row, column, value]\n","Index: []\n"]}],"source":["# run python code\n","%cd \"/content/drive/MyDrive/colab_wolfs_git_clones/lizard\"\n","!ls \n","!pwd\n","\n","%cd \"statistical_analysis/linear_regression/\"\n","!ls\n","!pwd\n","\n","!pip install reportlab\n","%run linear_regression_analyze_tp.py -i configuration.yaml"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"linear_regression_analyze_tp.ipynb","provenance":[{"file_id":"/v2/external/notebooks/intro.ipynb","timestamp":1639750401434}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}