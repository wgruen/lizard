{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"xcHo9ZaMw7SY","executionInfo":{"status":"ok","timestamp":1639946401805,"user_tz":300,"elapsed":13429,"user":{"displayName":"Wolfgang Gruen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01742609913198784862"}},"outputId":"8036c1ac-de15-47b1-a6e5-0d8688bedeb1","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting reportlab\n","  Downloading reportlab-3.6.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.7 MB)\n","\u001b[K     |████████████████████████████████| 2.7 MB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: pillow>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from reportlab) (7.1.2)\n","Installing collected packages: reportlab\n","Successfully installed reportlab-3.6.3\n","/content\n","sample_data\n"]}],"source":["from numpy import exp, array, random, dot, round\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd \n","import os\n","\n","!pip install reportlab \n","from reportlab.pdfgen import canvas\n","from reportlab.platypus import *\n","from reportlab.lib import colors\n","from reportlab.lib.pagesizes import letter\n","from reportlab.lib.styles import ParagraphStyle, getSampleStyleSheet\n","\n","styles = getSampleStyleSheet()\n","\n","#current dir\n","cwd = os.getcwd()\n","print(cwd)\n","\n","! ls"]},{"cell_type":"markdown","metadata":{"id":"JOASe4zdw7Sb"},"source":["### Read the data "]},{"cell_type":"raw","metadata":{"id":"vtxmc8PIw7Sc"},"source":["This file contains index of words appearing documents.\n","Each line presents a document\n","In each line, there are pairs with two data points. The first data point is an index to a word in the vocabulary file. The second data point is the the number of times that specific word appears in the document.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B4Yq_m13w7Sd","executionInfo":{"status":"error","timestamp":1639946402154,"user_tz":300,"elapsed":361,"user":{"displayName":"Wolfgang Gruen","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"01742609913198784862"}},"colab":{"base_uri":"https://localhost:8080/","height":200},"outputId":"de43f82f-3a06-43f0-f3e5-42ffa1efab10"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-2f9941c70efb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/nyt_data.txt'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"'\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocuments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './data/nyt_data.txt'"]}],"source":["with open('./data/nyt_data.txt') as f:\n","    documents = f.readlines()\n","documents = [x.strip().strip('\\n').strip(\"'\") for x in documents] "]},{"cell_type":"markdown","metadata":{"id":"opgrrsmHw7Se"},"source":["### Read the Vocabulary (dictionary)"]},{"cell_type":"raw","metadata":{"id":"LEWyRLDEw7Se"},"source":["This file contains vocabs. Each row has one vocab: \n","Index <space> vocabulary"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Psadqi3qw7Sg"},"outputs":[],"source":["with open('data/nyt_vocab.dat') as f:\n","    vocabs = f.readlines()\n","vocabs = [x.strip().strip('\\n').strip(\"'\") for x in vocabs] "]},{"cell_type":"markdown","metadata":{"id":"JF4sOt2-w7Sh"},"source":["### Create matrix for documents"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXp8Ml2Yw7Si"},"outputs":[],"source":["'''create matrix X'''\n","numDoc = 8447\n","numWord = 3012 \n","X = np.zeros([numWord,numDoc])\n","\n","for col in range(len(documents)):\n","    for row in documents[col].split(','):\n","        X[int(row.split(':')[0])-1,col] = int(row.split(':')[1])"]},{"cell_type":"raw","metadata":{"id":"hQ_t1mMMw7Sj"},"source":["Two matricies\n","\n","W is a matrix showing the ranking for each vocab\n","x the is a row for each vocab\n","y is the ranking for each vocab\n","\n","\n","H is a matrix for document ranking \n","x is the rank of each document\n","Initialize them with zeros\n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wk0Hj51Hw7Sk"},"outputs":[],"source":["'''randomly initialize W and H with nonnegative values'''\n","rank = 25\n","T = 100\n","W = np.zeros([numWord,rank])\n","H = np.zeros([rank,numDoc])\n","\n","for row in range(numWord):\n","    W[row] = np.random.rand(rank)\n","for row in range(rank):\n","    H[row] = np.random.rand(numDoc)\n","    \n","    \n","    "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ix3qe7Baw7Sl"},"outputs":[],"source":["'''setting divergence penalty''' #iterate values in H, then in W\n","d_iter = np.zeros(100)\n","\n","for iteration in range(100):\n","    \n","    '''iterate all values in H'''\n","    m1 = np.dot(W.T,X)\n","    m2 = np.dot(W,H)\n","    m3 = np.dot(m2.T,W)\n","    second = np.divide(m1,m3.T + 0.0000000000000001)\n","\n","    for k in range(rank):\n","        for j in range(numDoc):\n","            H[k,j] = np.multiply(H[k,j], second[k,j])\n","    \n","    '''iterate all values in W'''\n","    n1 = np.dot(H,X.T)\n","    n2 = np.dot(W,H)\n","    n3 = np.dot(n2,H.T)\n","    third = np.divide(n1.T,n3 + 0.0000000000000001)\n","\n","    for i in range(numWord):\n","        for k in range(rank):\n","            W[i,k] = np.multiply(W[i,k], third[i,k])\n","        \n","    '''plot objective function'''\n","#     D = np.multiply(X, np.log(1/(n2 + 0.0000000000000001))) + n2\n","#     d_iter[iteration] = np.sum(D)\n","    D = np.multiply(X,np.log(n2+0.0000000000000001)) - n2\n","    d_iter[iteration] = -np.sum(D)\n","\n","fig= plt.figure(figsize = (15,6))\n","ax = fig.add_subplot(1,1,1)\n","ax.plot(range(100),d_iter[:100])\n","plt.title('Plot of divergence objective in 100 iterations')\n","plt.ylabel('$D(X||WH)$')\n","plt.xlabel('iteration $t$')\n","plt.show()\n","\n","\n","# ### b. Ten words with the largest weight.\n","\n","# In[502]:\n","\n","'''normalize each column to sum to zero'''\n","W_normed = W / np.sum(W,axis=0)\n","\n","\n","# In[511]:\n","\n","'''for each column of W, list the 10 words having the largest weight and show the weight'''\n","pd.set_option('display.max_rows', 50)\n","pd.set_option('display.max_columns', 50)\n","pd.set_option('display.width', 120)    \n","vList = []\n","\n","\n","if not os.path.exists(\"output\"):\n","    os.mkdir(\"output\")\n","doc_summary  = SimpleDocTemplate(\"output/news_charaterization_summary.pdf\"+ \"_summary.pdf\", pagesize=letter)\n","element = []\n","header = Paragraph(\"\\nSummary of Analysis Run\", styles[\"Heading1\"])\n","element.append(header)\n","    \n","\n","\n","for topic in range(rank):\n","    v = pd.DataFrame(vocabs)\n","    v[1] = W_normed[:,topic].round(6)\n","    v = v.sort_values([1, 0], ascending=[0,1]).rename(index=int, columns={0: \"Topic {}\".format(topic+1), 1: \"Weight\"}).head(10)\n","    v = v.reset_index(drop=True)\n","    vList.append(v)\n","    \n","#    lista = [df.columns[:,].values.astype(str).tolist()] + df.values.tolist()\n","#    t1 = Table(lista)        \n","#    element.append(t1)\n","#    element.append(v)\n","     \n","\n","    \n","for num in [5,10,15,20,25]:\n","    print('\\n',(pd.concat(vList[num-5:num], axis=1)),'\\n')\n","    print(\"lineline\")\n","    print(vList[num-5:num])\n","    print(\"linelineline\")\n","    t1 = Table(vList[num-5:num])  \n","    element.append(t1)\n","    t11 = vList[num-5:num]\n","    print(\"t11\")\n","    print(t11)\n","    element.append(Table(t11))\n","    \n","    \n","doc_summary.build(element)\n","    \n","# sort by standard deviation  \n","#header = Paragraph(\"\\nSorted by Interquartile Range\", styles[\"Heading2\"])Analysis\n","#element.append(header)\n","    \n","\n","#df = df.sort_values(by=[\"interquartile_range\"])\n","#    lista = [df.columns[:,].values.astype(str).tolist()] + df.values.tolist()\n","#    t1 = Table(lista)        \n","#    element.append(t1)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"news_charaterization.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}